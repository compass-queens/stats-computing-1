---
title: "SC1_Proj"
author: "Alessio"
date: "13 January 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
```

```{r, include=FALSE}
library(ggplot2)
library(magrittr)
library(dplyr)
library(corrplot)
library(tidyr)
library(readr)
library(devtools)
library(cowplot)
library(reshape2)
#library(tsne)
install_github('alessio-b-zak/sc1-svm-example-package')
library(alessiosvm)
```

# Dataset
The dataset is
TODO: describe

```{r}
data <- read_csv("../data/data.csv")
glimpse(data)
colnames(data)[3:32] <- c('radius_m','texture_m', 'perim_m','area_m','smooth_m','compact_m','concav_m','concav_pt_m','symmetry_m','frac_dim_m','radius_se','texture_se', 'perim_se','area_se','smooth_se','compact_se','concav_se','concav_pt_se','symmetry_se','frac_dim_se','radius_w','texture_w', 'perim_w','area_w','smooth_w','compact_w','concav_w','concav_pt_w','symmetry_w','frac_dim_w')
```

#Dataset Preprocessing Visualisation and Exploration

```{r}
colSums(is.na(data))
```


```{r}
data %<>% mutate_at(vars(diagnosis), factor)
```

```{r}
train <- data %>% sample_frac(0.8)
test <- anti_join(data,train, by='id')

# need ids for later
id_train <- train$id
id_test <- test$id
```

```{r}
data %<>%  
  dplyr::select(-c(id, X33))
train %<>%  
  dplyr::select(-c(id, X33))
test %<>%  
  dplyr::select(-c(id, X33))
```

```{r}
sum(is.na(data))
```

```{r}
training_data <- train[2:dim(train)[2]] 
training_classes <- train[1] 

test_data <- test[2:dim(test)[2]] 
test_classes <- test[1] 
```

```{r echo=FALSE}
corr <- training_data[,-1] %>% 
          cor() %>% 
           round(1)

corrplot(corr, method='circle',order='hclust',tl.col = "black")
```

```{r fig.height=40, fig.width=30, echo=FALSE}
train %>% 
  group_by(diagnosis) %>% 
  gather(key="var",value="value",-diagnosis) %>% 
  ggplot(aes(x=value))+
  facet_wrap(~var,scales='free',nrow=6)+
  geom_density(aes(fill=diagnosis),alpha=0.4)+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position="bottom")
```

```{r echo=FALSE}
g1<-ggplot(training_classes,aes(x=diagnosis,fill=diagnosis))+
  geom_bar(aes(y=(..count..)/sum(..count..)))+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position='none')
  

g2<-ggplot(test_classes,aes(x=diagnosis,fill=diagnosis))+
  geom_bar(aes(y=(..count..)/sum(..count..)))+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position='none')
plot_grid(g1,g2)
```

```{r}
confusion_plot <- function(actual,predicted){
  confusion_matrix <- as.data.frame(table(actual,predicted))
  g <-ggplot(confusion_matrix,aes(x=actual,y=predicted))+
    geom_tile(aes(fill=Freq))+
    geom_text(aes(label=sprintf("%1.0f", Freq)),color="white",fontface="bold")+
   labs(x="Actual class",y="Predicted class")+
    theme_minimal()
  return(g)
}
```

# Dimensionality Reduction and Feature Selection

## PCA

Code
```{r}
normalise_z <-  function(X){
  mean_cols <- colMeans(X)
  sd_cols <- apply(X, 2, sd)
  mean_normalised_X <- t(apply(X, 1, function(x){x - mean_cols}))
  normalised_X <- t(apply(mean_normalised_X, 1, function(x){x / sd_cols}))
  return(normalised_X)
}


pca <- function(X, number_components_keep) {
  normalised_X <- normalise_z(X)

  corr_mat <- t(normalised_X) %*% normalised_X

  eigenvectors <- eigen(corr_mat, symmetric=TRUE)$vectors

  reduced_data <-  X %*% eigenvectors[,1:number_components_keep]
  relevant_eigs <-  eigenvectors[,1:number_components_keep]
  returnds <- list(reduced_data, relevant_eigs)
  names(returnds) <- c("reduced_data", "reduction_matrix")
  return(returnds)
}
```

Apply to dataset
```{r}
pca_result <- pca(as.matrix(training_data), 2)
reduced_training_data <- data.frame(cbind(pca_result$reduced_data, training_classes))

ggplot(data=reduced_training_data, aes(x=X1, y=X2)) + geom_point(aes(colour=diagnosis))
```


<!--  tSNE -->
<!-- TODO: try different perplexity parameters -->
<!-- ```{r} -->
<!-- reduced_training_data <- tsne::tsne(training_data) -->

<!-- reduced_training_data <- data.frame(cbind(reduced_training_data, training_classes)) -->
<!-- ggplot(data=reduced_training_data, aes(x=X1, y=X2)) + geom_point(aes(colour=diagnosis)) -->
<!-- ``` -->

  - Correlation Feature Selection
  - LDA

# Classification

TODO: Mathematical description 
 
TODO: Basic Code describing implementation 

# SVM
  
```{r}
soft_margin_svm_plotter <- function(w, b) {
  plotter <- function(x) {
    return(1/w[2]   * -(b + (w[1]*as.numeric(x))))
  }
  return(plotter)
}
```

```{r}
factor_to_label <- function(x)  {
  if(as.character(x) == "M")  {
    return(1)
  }
  else {
    return(-1)
  }
}
```

```{r}
label_to_factor <- function(x) {
  if(x == 1) {
    return(as.factor("M"))
  }
  else{
    return(as.factor("B"))
  }
}
numeric_test_labels <- apply(test_classes, 1, factor_to_label)
numeric_training_labels <- apply(training_classes, 1, factor_to_label)
```


Use PCA then do SVM 
```{r}
model <- svm(X=pca_result$reduced_data, 
             classes=numeric_training_labels, 
             C=100000, margin_type='soft', 
             kernel_function = linear_kernel, 
             feature_map = linear_basis_function)


reduced_prediction_fn <- model$prediction_function

pca_reduced_prediction_fn <- function(x) {
  p <- x %*% pca_result$reduction_matrix
  reduced_prediction_fn(t(p))
}
```
 
```{r}
predictions_svm <- apply(as.matrix(test_data),1, pca_reduced_prediction_fn)
accuracy_calc(numeric_test_labels, predictions_svm)
```

```{r}
svm_plotter <- soft_margin_svm_plotter(model$params$w, model$params$b)
embedded_test_data <- data.frame(cbind(as.matrix(test_data) %*% pca_result$reduction_matrix), test_classes)

ggplot(embedded_test_data, aes(x=as.numeric(X1), y=as.numeric(X2))) + 
  geom_point(aes(colour=
                   diagnosis)) + 
  stat_function(fun=svm_plotter)
```


# Naive Bayes
## Mathematical setting

Let $y$ be the class label that we want to assign to an observation $\boldsymbol{x}=(x_1,\cdots x_d)$, where $x_1,\cdots x_d$ are the features. The probability of an observation having label $y$ is given by Bayes rule,
\begin{align*}
P(y|x_1,\cdots,x_d)&=\frac{P(x_1,\cdots,x_d|y_k)P(y)}{P(x_1,\cdots,x_d)}\\
&\propto P(x_1,\cdots,x_d|y_k)P(y).
\end{align*}

The prior class probability $P(y)$ can be easily obtained by the proportion of observation that are in the given class.


The main assumption is that every feature is conditionally independent given the class label $y$. The reason why this classifier is called *naive* is that very often this assumption is not actually realistic.


This assumption simplifies the posterior to
$$P(y|x_1,\cdots,x_d) \propto P(y)\prod_{i=1}^d P(x_i|y).$$
There are various types of Naive Bayes classifiers based on the type of features. In our case, since we have continuous variables we assume that all features are normally distributed. Therefore, the conditional probabilities can be calculated as
$$P(x_i|y)=\frac{1}{\sqrt{2\pi\sigma_y^2}}exp\left(-\frac{(x_i-\mu_y)^2}{2\sigma_y^2}\right)$$

Finally, to assign the class to an observation we use the Maximum A Posteriori decision rule. For every observation, we pick the class the has the highest probability
$$y=\underset{y}{\text{argmax}}P(y)\prod_{i=1}^dP(x_i|y).$$

## Implementation

*Here are some code snippets just to illustrate how these theoretical aspects are implemented. The full code can be found in the package.*

The observations are stored as rows in $X$ and the corresponding class labels are entires in the column matrix $y$.

First we calculate the prior class probabilities based on the number of observations in each class.

```{r eval = FALSE}
n <- dim(X)[1]
d <- dim(X)[2]
classes <- sort(unique(y)[, 1])
k <- length(classes)

prior <- rep(0, k)
for (i in 1:k) {
  prior[i] <- sum(y == classes[i]) / n
}
```

Then we create an array of the mean and sd of the data split by clasess and features.

```{r eval = FALSE}
summaries <- array(rep(1, d * k * 2), dim = c(k, d, 2))
for (i in 1:k) {
  X_k <- X[which(y == (i - 1)), ]
  summaries[i, , 1] <- apply(X_k, 2, mean)
  summaries[i, , 2] <- apply(X_k, 2, sd)
}
```

Finally, the predictions are obtained by taking the largest posterior class probability. Note that in order to avoid underflow, we take the maximum of the *log* posterior class probabilities. 

```{r eval = FALSE}
probs <- matrix(rep(0, n * k), nrow = n)
for (obs in 1:n) {
  for (class in 1:k) {
    class_prob <- log(prior[class])
    for (feat in 1:d) {
      mu <- summaries[class, feat, 1]
      sd <- summaries[class, feat, 2]
      cond <- dnorm(x_new[obs, feat], mu, sd, log = TRUE)
      class_prob <- class_prob + cond
      }
     probs[obs, class] <- class_prob
  }
}

pred <- apply(probs, 1, which.max)
```

## Fit model to dataset
```{r}
install_github("andreabecsek/NaiveBayes")
library(NaiveBayes)
```

<!-- Convert class labels from $-1,1$ to $0,1$. -->
<!-- ```{r} -->
<!-- numeric_training_labels[numeric_training_labels==-1]<-0 -->
<!-- numeric_training_labels %<>%  -->
<!--   as.matrix() -->

<!-- numeric_test_labels[numeric_training_labels==-1]<-0 -->
<!-- numeric_test_labels %<>%  -->
<!--   as.matrix() -->
<!-- ``` -->

```{r}
levels(training_classes$diagnosis) <- c(0,1)
training_classes %<>% as.matrix
mode(training_classes) <- 'numeric'

levels(test_classes$diagnosis) <-c(0,1)
test_classes %<>% as.matrix
mode(test_classes) <- 'numeric'
```

Fit the Naive Bayes model to the data, calculate predictions and check the accuracy using.
```{r}
model_naive <- naive_bayes(training_data,training_classes)

predictions_naive <- predict(model_naive,as.matrix(test_data))

confusion_plot(test_classes,predictions_naive)
```

Merge all predictions
```{r}
id <- seq(length(id_test))
all_predictions <- cbind(id,numeric_test_labels,predictions_naive,predictions_svm)
colnames(all_predictions) <- c('id','actual','naive','svm')
all_predictions[all_predictions==-1] <-0
all_predictions %<>% as.data.frame()
```

```{r}
errors <- all_predictions %>% 
  mutate(naive=naive==actual) %>% 
  mutate(svm=svm==actual) %>% 
  dplyr::select(-actual)
```


```{r}
a <- errors %>% 
  melt(id='id')

ggplot(a,aes(x=id,y=variable,fill=value))+
  geom_raster()+
  theme_minimal()
```



TODO: analyse results
  
  - Naive Bayes
  - SVM
  - Logistic Regression

#Conclusion
  - Evaluation of results
  - Discuss outliers
  
TODO: create outlier plot