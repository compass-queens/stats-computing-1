---
title: "Breast Cancer Project"
output: pdf_document
urlcolor: blue
header-includes:
- \usepackage{amsmath}
- \usepackage{bm}
- \usepackage{algorithm2e}
---
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\vbeta}{\vect{\beta}}
\newcommand{\vx}{\vect{x}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vzero}{\vect{0}}
\newcommand{\vSigma}{\vect{\Sigma}}
\newcommand{\vtheta}{\vect{\theta}}
\newcommand{\uniform}{\mathcal{U}(0, 1)}
```{r, include=FALSE}
library(ggplot2)
library(magrittr)
library(dplyr)
library(corrplot)
library(tidyr)
library(readr)
library(devtools)
library(tsne)
library(devtools)
library(ggpubr)
library(reshape2)
library(gridExtra)
library(knitr)
library(latex2exp)
# install_github("alessio-b-zak/sc1-svm-example-package")
# install_github("MauroCE/LogisticRegression")
# install_github("andreabecsek/NaiveBayes")
library(alessiosvm)
library(LogisticRegression)
library(NaiveBayes)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

```{r  echo=FALSE}
theme_set(theme_minimal())
```

# Introduction
One of the most common types of cancer diagnosed in women is breast cancer. There are multiple tests that people are subjected to, but one of the most indicative ones is fine needle aspiration which involes extracting a sample of cells to be examined under a microscope. Multiple numerical metrics are computed from the obtained images. The aim is to use the extracted metrics to make accurate diagnoses. 

The dataset consists of $569$ images which have been processed as described and a total of $30$ variables have been computed for each observation. The aim of this report is to implement a number of classification algorithms, use them to obtain predictions, and compare their performances. The features describe characterstics of the cell nucleus wich three different statistics computed for each feature. The three statistics are the largest value computed of the feature computed for each image indicated with a "_w" appended to the feature name, the mean value of the feature for the image indicated with a "_m" appended, and the standard error. The features computed are the radius of the nuclei, the texture of the nuclei, area, perimeter, smoothness, compactness, concavity, concave points,  symmetry, and fractal dimension.


# Exploratory analysis
We begin by exploring the data to gain some understanding of the features and their relationships to inform further work. We do the majority of the data manipulation and analysis within this report using the tidyverse library. We begin by loading the data as a tibble:

```{r}
data <- read_csv("../data/data.csv")
```

```{r, echo=FALSE}
colnames(data)[3:32] <- c(
  "radius_m", "texture_m", "perim_m", "area_m", "smooth_m", "compact_m",
  "concav_m", "concav_pt_m", "symmetry_m", "frac_dim_m", "radius_se",
  "texture_se", "perim_se", "area_se", "smooth_se", "compact_se",
  "concav_se", "concav_pt_se", "symmetry_se", "frac_dim_se", "radius_w",
  "texture_w", "perim_w", "area_w", "smooth_w", "compact_w", "concav_w",
  "concav_pt_w", "symmetry_w", "frac_dim_w"
)
```

After renaming some variables to some more manageable variables we next have to ensure the hygiene of our data. Through manual inspection it is clear that our data is already in the tidy data format so all that remains is to see if any corrupted data is present:

```{r}
colSums(is.na(data))
```

From this we can see that the feature parsed as `X33` is not present for any observation and therefore can be removed without any difficulties using `dplyr`

```{r}
data %<>%
  dplyr::select(-c(X33))
```

In order to make the data easier to work with we also need to set the response, the diagnosis of the patient, to be a factor.
```{r}
data %<>% mutate_at(vars(diagnosis), factor)
```

The next step before analysing the data is to produce a test/train split in order to evaluate how well our models and methods will generalise which we can do again using `dplyr`. We choose an 80/20 train/test split.

```{r}
train <- data %>% sample_frac(0.8)
test <- anti_join(data, train, by = "id")
```

We also remove the labels and store them elsewhere so that we have the raw data input to analyse.

```{r}
id_train <- train$id
id_test <- test$id

data %<>%
  dplyr::select(-c(id))
train %<>%
  dplyr::select(-c(id))
test %<>%
  dplyr::select(-c(id))
```


```{r, echo=FALSE}
training_data <- train[2:dim(train)[2]]
training_classes <- train[1]
test_data <- test[2:dim(test)[2]]
test_classes <- test[1]
```

Before continuing with our analysis we need to make sure that the train/test split is representative of the entire data. Below is a plot of the class distribution in the training set on the left and the test set on the right. As can be seen these are fairly even and therefore this split is suitable for analysis.
```{r echo=FALSE, fig.cap='Class distribution of the training (left) and test data (right).'}
g1 <- ggplot(training_classes, aes(x = diagnosis, fill = diagnosis)) +
  geom_bar(aes(y = (..count..) / sum(..count..))) +
  theme_minimal()+
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "none"
  )


g2 <- ggplot(test_classes, aes(x = diagnosis, fill = diagnosis)) +
  geom_bar(aes(y = (..count..) / sum(..count..))) +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "none"
  )
ggarrange(g1, g2)
```

We now want to try and discover if our data has any exploitable structure that we can use for classification. We can examine the density plots of each feature for each class against each other to see if there are features which have substantially different plots.

```{r fig.height=40, fig.width=30, echo=FALSE, fig.cap='Density plots for each feature split by diagnosis.'}
train %>%
  group_by(diagnosis) %>%
  gather(key = "var", value = "value", -diagnosis) %>%
  ggplot(aes(x = value)) +
  facet_wrap(~var, scales = "free", nrow = 6) +
  geom_density(aes(fill = diagnosis), alpha = 0.4) +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "bottom",
    axis.text = element_text(size = 35),
    axis.title = element_text(size = 35),
    text = element_text(size = 35),
    strip.text = element_text(size = 35)
  )
```

From these plots above we can see that many of the features are, from visual, inspection, approximately normal. Whilst this is not conclusive as per a test however this may justify some techniques for classification which rely on normality of the features (as in the choice of naive bayes below).

Many of the features display little difference in densities between classes however several of the features display noticeable difference which is promising for modelling.

The final plot we want to do is to check the pairwise correlations betwee all pairs of features. 
```{r echo=FALSE}
corr <- training_data[, -1] %>%
  cor() %>%
  round(1)
```

```{r fig.align='center', fig.cap='Correlation plot for all features.'}
corrplot(corr, method = "circle", order = "hclust", tl.col = "black")
```

This plot shows that a large number of our features are highly correlated which gives us some indication that we can perhaps predict some features from others and reduce the dimension of the data we are working with. Now that we have established that there appears to be some structure within our data and that there appears to be more important variables we can move on with an attempt to reduce the dimensionality of our data.
\newpage

# Dimensionality Reduction and Feature Selection

## PCA

For data of a non-trivial dimensionality it can be difficult to know where to begin with the modelling process as we may not have an intuitive idea of the underlying structure in our code. One such method of visualising the data in a lower space is principal component analysis (PCA). PCA aims to produce a set of linearly uncorrelated variables from our original set of variables of a reduced size. It does this by first taking the dataset represented as a matrix:

$$
X = \begin{pmatrix}
x_{1}^{T} \\
\vdots \\
x_{2}^{T}
\end{pmatrix}
$$
then we form a matrix normalised by the standard score by subtracting the column means from every column and dividing every column by the standard deviation for that column as in:
```{r}
normalise_z <- function(X) {
  mean_cols <- colMeans(X)
  sd_cols <- apply(X, 2, sd)
  mean_normalised_X <- t(apply(X, 1, function(x) {
    x - mean_cols
  }))
  normalised_X <- t(apply(mean_normalised_X, 1, function(x) {
    x / sd_cols
  }))
  return(normalised_X)
}
```
Giving us:
$$
Z = \begin{pmatrix} 
\frac{x_{11} - \mu_{1}}{\sigma_{1}} & ... & \frac{x_{1d} - \mu_{d}}{\sigma{d}} \\
\vdots & \vdots & \vdots \\
\frac{x_{n1} - \mu_{1}}{\sigma_{1}} & ... & \frac{x_{nd} - \mu_{d}}{\sigma{d}} \\
\end{pmatrix}
$$
Multiplying this by its transpose gives us the correlation matrix where the entry $\rho_{ij}$ is the correlation between observation $i$ and observation $j$. We can take the eigendecomposition of this matrix product to give us:

$$
Z^{T}Z = P \,\Sigma^{-1}P^{T}
$$
Where we assume that the diagonal $\Sigma$ is ordered by size. The eigenvectors corresponding to the largest eigenvalues represent the combinations of features which account for the highest variance. If we wish to visualise at dimension $k < d$ we can simply take the top $k$ eigenvectors as a matrix and multiply our data by this visualise our data in the reduced space. Code that does this can be found below.
```{r}
pca <- function(X, number_components_keep) {
  normalised_X <- normalise_z(X)

  corr_mat <- t(normalised_X) %*% normalised_X

  eigenvectors <- eigen(corr_mat, symmetric = TRUE)$vectors

  reduced_data <- X %*% eigenvectors[, 1:number_components_keep]
  relevant_eigs <- eigenvectors[, 1:number_components_keep]
  returnds <- list(reduced_data, relevant_eigs)
  names(returnds) <- c("reduced_data", "reduction_matrix")
  return(returnds)
}
```


When applying this to the dataset and keeping 2 components the following plot results:

```{r, echo=FALSE, fig.cap='Dataset reduced to two principal components.'}
pca_result <- pca(as.matrix(training_data), 2)
pca_reduced_training_data <- data.frame(cbind(pca_result$reduced_data, training_classes))

ggplot(data = pca_reduced_training_data, aes(x = X1, y = X2)) +
  geom_point(aes(colour = diagnosis))+
  xlab('Principal Component 1')+
  ylab('Principal Component 2')
```
  

As we can see from this plot there is clearly some structure within the data that we can exploit for classification. This reduced dataset and transformation matrix will be used in classification attempts in the next section.

\newpage

# Classification

In this section we will explore various approaches of classifying the data. This includes using SVMs, Naive Bayes, and frequentist and bayesian approaches to logistic regression. We begin with SVMs applied to the reduced dataset.

## SVM
From the application of PCA to the dataset we can see that, after reducing to 2 dimensions, the data appears to be almost linearly separable. Given this, an appropriate method of classifying the data would be to apply a soft-margin SVM to the reduced dimension data. Soft-margin SVMs solve the problem of classifying non-separable data by permitting a certain number of points to be incorrectly classifed however the number and the amount they violate the constraints by must be as small as possible. After manipulating the reformulated optimisation problem we end up with the optimisation problem
$$
\underset{\lambda}{\text{min}} \, \frac{\overline{\lambda}X X^{T}\overline{\lambda^{T}}}{4}
+ \lambda^{T}\bm{1}
$$
$$
\text{such that } 0 \leq \lambda_{i} \leq C
$$
$$
\text{and } \sum\limits_{i}^{n} \lambda_{i}y_{i} = 0
$$

where
$$
X = \begin{pmatrix}
x_{1}^{T} \\
\vdots \\
x_{n}^{T}
\end{pmatrix} \text{ and } 
\overline{\lambda} = [\lambda_{1} \cdot y_{i}, ... , \lambda_{n} \cdot y_{n}] \text{ and } \bm{1} = [1, ..., 1] \in \mathbb{R^{n}}
$$
For some predefined $C$. This is a quadratic programming problem with linear constraints which can be solved using the R package `quadprog`with the function `solve.QP`. From its documentation, this function can solve (for $b$) problems in the form $\underset{b}{\text{min}}(-d^{T}b + \frac{1}{2} b^{T}Db)$ with the constraints that $A^{T} b \geq b_0$. By transforming the above problem into this format we can implement soft-margin SVM using the following code
```{r, eval=FALSE}
train_soft_svm <- function(X, y, C) {
  
  num_observation <- nrow(X)
  dim_num <- ncol(X)

  Dmat2 <- diag(y) * X %*% t(X) %*% diag(y)
  diag(Dmat2) <- diag(Dmat2) + 1e-6
  dv2 <- rep(1, num_observation)
  
  A2 <- rbind( y,diag(num_observation)) 
  A2 <- rbind(A2, -1*diag(num_observation))
  
  bv2 <- c(c(0), rep(0, num_observation), rep(-C, num_observation) )
  model <- solve.QP(Dmat2, dv2, t(A2), bv2, meq = 1) 
}
```

In order to recover $w$ and $b$ from $\lambda$ we use the relationship
$$
w = \sum\limits_{i=0}^{n-1}\lambda_{i}x_{i}^{T}y_{i}
$$
and
$$
b = \text{mean}(\sum\limits_{i=0}^{k} y_{i} - w \cdot x_{i}) \, . \, \forall \, i . 0 < \lambda_{i} < C
$$

Which can be made as functions in R as so:

```{r, eval=FALSE}
calculate_b <- function(w, X, y, a, C) {
  ks <- sapply(a, function(x) {
    return(x > 0 && x < C)
  })
  indices <- which(ks)
  sum_bs <- 0
  for (i in indices) {
    sum_bs <- sum_bs + (y[i] - w %*% X[i, ])
  }
  return(sum_bs / length(indices))
}


recover_w <- function(a, y, X) {
  colSums(diag(a) %*% diag(y) %*% X)
}
```


From the parameters we can recover the equation of the line corresopnding the decision boundary which can later be used for plotting
```{r}
soft_margin_svm_plotter <- function(w, b) {
  plotter <- function(x) {
    return(1 / w[2] * -(b + (w[1] * as.numeric(x))))
  }
  return(plotter)
}
```


```{r, echo=FALSE}
factor_to_label <- function(x)  {
  if(as.character(x) == "M")  {
    return(1)
  }
  else {
    
    return(-1)
  }
}
```

```{r, echo=FALSE}
label_to_factor <- function(x) {
  if (x == 1) {
    return(as.factor("M"))
  }
  else {
    return(as.factor("B"))
  }
}
numeric_test_labels <- apply(test_classes, 1, factor_to_label)
numeric_training_labels <- apply(training_classes, 1, factor_to_label)
```

Using the extracted parameters $w$ and $b$ we can create a classification function

$$
f(x) = \text{sign}(w \cdot x) + b
$$

We are now able to define a function which first embeds the vector in the reduced space and then predicts the class based upon the prediction function produced by the parameters found by the SVM. Below is code that will do the following based upon a package written for this assignment found [here](https://github.com/alessio-b-zak/sc1-svm-example-package)
```{r}
model <- svm(
  X = pca_result$reduced_data,
  classes = numeric_training_labels,
  C = 100000, margin_type = "soft",
  kernel_function = linear_kernel,
  feature_map = linear_basis_function
)


reduced_prediction_fn <- model$prediction_function

pca_reduced_prediction_fn <- function(x) {
  p <- x %*% pca_result$reduction_matrix
  reduced_prediction_fn(t(p))
}
```

```{r}
pca_reduced_test_data <- t(apply(as.matrix(test_data), 1, function(x) x %*%
                                   pca_result$reduction_matrix))
```
 

```{r, echo=FALSE}
pred_svm <- apply(as.matrix(test_data),1, pca_reduced_prediction_fn)
```


```{r echo = FALSE}
confusion_plot <- function(actual, predicted, method='') {
  confusion_matrix <- as.data.frame(table(actual, predicted))
  g <- ggplot(confusion_matrix, aes(x = actual, y = predicted)) +
    geom_tile(aes(fill = Freq)) +
    geom_text(aes(label = sprintf("%1.0f", Freq)), color = "white", fontface = "bold",size=6) +
    labs(x = "Actual", y = "Predicted",title=method) +
    theme_minimal()+
    theme(legend.position = 'none',
          plot.title = element_text(hjust=0.5))
  return(g)
}
```

```{r fig.width=3,fig.height=3, fig.align='center', fig.cap='Confusion plot for SVM predictions.'}
confusion_plot(numeric_test_labels,pred_svm,'SVM')
```

When running the trained SVM prediction function on the test set we achieve `r accuracy_calc(numeric_test_labels, pred_svm)` %. We can plot this code as below

```{r, echo=FALSE, fig.cap='SVM decision boundary in the reduced space.'}
svm_plotter <- soft_margin_svm_plotter(model$params$w, model$params$b)
embedded_test_data <- data.frame(cbind(as.matrix(test_data) %*%
                                pca_result$reduction_matrix), test_classes)

ggplot(embedded_test_data, aes(x = as.numeric(X1), y = as.numeric(X2))) +
  geom_point(aes(colour = diagnosis)) +
  stat_function(fun = svm_plotter)+
  xlab('Principal Component 1')+
  ylab('Principal Component 2')
```


\newpage
# Naive Bayes
## Mathematical setting

Let $y$ be the class label that we want to assign to an observation $\boldsymbol{x}=(x_1,\cdots x_d)$, where $x_1,\cdots x_d$ are the features. The probability of an observation having label $y$ is given by Bayes rule,
\begin{align*}
P(y|x_1,\cdots,x_d)&=\frac{P(x_1,\cdots,x_d|y_k)P(y)}{P(x_1,\cdots,x_d)}\\
&\propto P(x_1,\cdots,x_d|y_k)P(y).
\end{align*}

The prior class probability $P(y)$ can be easily obtained by the proportion of observation that are in the given class.


The main assumption is that every feature is conditionally independent given the class label $y$. The reason why this classifier is called *naive* is that very often this assumption is not actually realistic.


This assumption simplifies the posterior to
$$P(y|x_1,\cdots,x_d) \propto P(y)\prod_{i=1}^d P(x_i|y).$$
There are various types of Naive Bayes classifiers based on the type of features. In our case, since we have continuous variables we assume that all features are normally distributed. Therefore, the conditional probabilities can be calculated as
$$P(x_i|y)=\frac{1}{\sqrt{2\pi\sigma_y^2}}exp\left(-\frac{(x_i-\mu_y)^2}{2\sigma_y^2}\right)$$

Finally, to assign the class to an observation we use the Maximum A Posteriori decision rule. For every observation, we pick the class the has the highest probability
$$y=\underset{y}{\text{argmax}}P(y)\prod_{i=1}^dP(x_i|y).$$

## Implementation

*Here are some code snippets just to illustrate how these theoretical aspects are implemented. The full code can be found in the package.*

The observations are stored as rows in $X$ and the corresponding class labels are entires in the column matrix $y$.

First we calculate the prior class probabilities based on the number of observations in each class.

```{r eval = FALSE}
n <- dim(X)[1]
d <- dim(X)[2]
classes <- sort(unique(y)[, 1])
k <- length(classes)

prior <- rep(0, k)
for (i in 1:k) {
  prior[i] <- sum(y == classes[i]) / n
}
```

Then we create an array of the mean and sd of the data split by clasess and features.

```{r eval = FALSE}
summaries <- array(rep(1, d * k * 2), dim = c(k, d, 2))
for (i in 1:k) {
  X_k <- X[which(y == (i - 1)), ]
  summaries[i, , 1] <- apply(X_k, 2, mean)
  summaries[i, , 2] <- apply(X_k, 2, sd)
}
```

Finally, the predictions are obtained by taking the largest posterior class probability. Note that in order to avoid underflow, we take the maximum of the *log* posterior class probabilities. 

```{r eval = FALSE}
probs <- matrix(rep(0, n * k), nrow = n)
for (obs in 1:n) {
  for (class in 1:k) {
    class_prob <- log(prior[class])
    for (feat in 1:d) {
      mu <- summaries[class, feat, 1]
      sd <- summaries[class, feat, 2]
      cond <- dnorm(x_new[obs, feat], mu, sd, log = TRUE)
      class_prob <- class_prob + cond
    }
    probs[obs, class] <- class_prob
  }
}

pred <- apply(probs, 1, which.max)
```

## Fit model to dataset
Fitting the model to the full dataset and the reduced dataset we obtain the following confusion matrices.
```{r echo=FALSE}
levels(training_classes$diagnosis) <- c(0, 1)
training_classes %<>% as.matrix
mode(training_classes) <- "numeric"

levels(test_classes$diagnosis) <- c(0, 1)
test_classes %<>% as.matrix
mode(test_classes) <- "numeric"
```

```{r echo=FALSE}
model_naive <- naive_bayes(training_data, training_classes)

pred_naive <- predict(model_naive, as.matrix(test_data))

calc_accuracy <- function(ytest, yhat) sum(drop(yhat) == drop(ytest)) / length(drop(ytest))

acc_naive <- calc_accuracy(test_classes, pred_naive)
```

```{r echo=FALSE}
model_naive_pca <- naive_bayes(pca_result$reduced_data, training_classes)

pred_naive_pca <- predict(model_naive_pca, pca_reduced_test_data)

acc_naive_pca <- calc_accuracy(test_classes, pred_naive_pca)
```

```{r fig.width=6, fit.height=3, fig.align='center', fig.cap='Confusion plots for the Naive Bayes predictions on the full dataset (left) and reduced (right).'}
g1 <- confusion_plot(test_classes,pred_naive,'Naive Bayes on full dataset')
g2 <- confusion_plot(test_classes,pred_naive_pca,'Naive Bayes on reduced dataset')
ggarrange(g1,g2)
```

Therefore, fitting a Naive Bayes model on the full dataset has an accuracy of `r acc_naive`, while on the reduced dataset it obtains `r acc_naive_pca`.

\newpage

# Logistic Regression
## Mathematical Setting
Let $Y_i\mid \vx_i \sim \text{Bernoulli}(p_i)$ with $p_i = \sigma(\vx_i^\top \vbeta)$ where $\sigma(\cdot)$ is the **sigmoid function**. The joint log-likelihood is given by
$$
\ln p(\vy\mid \vbeta) = \sum_{i=1}^n y_i \ln(p_i) + (1 - y_i)\ln(1 - p_i)=-\sum_{i=1}^n\ln\left(1 + \exp((1 - 2y_i)\vx_i^\top\vbeta)\right)
$$

### Maximum Likelihood Estimation
Maximizing the likelihood is equivalent to minimizing the negative log-likelihood. Minimizing the negative log likelihood is equivalent to solving the following optimization problem

$$
\min_{\vbeta}\sum_{i=1}^n\ln\left(1 + \exp((1 - 2y_i)\vx_i^\top\vbeta)\right)
$$

### Maximum-A-Posteriori and Ridge Regularization
We can introduce an isotropic Gaussian prior on **all** the coefficients $p(\vbeta) = N(\vzero, \sigma_{\vbeta}^2 I)$. Maximizing the posterior $p(\vbeta \mid \vy)$ is equivalent to minimizing the negative log posterior $-\ln p(\vbeta\mid \vy)$ giving

$$
\min_{\vbeta} \sigma^2_{\vbeta}\sum_{i=1}^n\ln\left(1 + \exp((1 - 2y_i)\vx_i^\top\vbeta)\right) + \frac{1}{2}\vbeta^\top\vbeta
$$

### Newton's Method
For stability, we add a learning rate, which is in practice often set to $\alpha=0.1$. The
iterations for Maximum Likelihood take the form of
$$
\vbeta_{k+1} \leftarrow \vbeta_k +  \alpha(X^\top D X)^{-1} X^\top(\vy - \sigma(X\vbeta_k))
$$
whereas for MAP take the form 
$$
\vbeta_{k+1} \leftarrow \vbeta_k + \alpha \left[\sigma^2_{\vbeta} X^\top D X + I\right]^{-1}\left(\sigma^2_{\vbeta} X^\top (\vy - \sigma(X\vbeta_k)) - \vbeta_k\right)
$$
In practice, for example in MLE, we would solve the corresponding system for $\vect{d}_k$
$$
(X^\top D X)\vect{d}_k = \alpha X^\top(\vy - \sigma(X\vbeta_k))
$$
and then perform the update
$$
\vbeta_{k+1}\leftarrow \vbeta_k + \vect{d}_k
$$

## Implementation and Results
The cost functions for maximum likelihood estimation and for maximum a posteriori can be implemented as follows.
```{r}
mle_cost <- function(beta) sum(log(1 + exp((1 - 2*y) * (X %*% beta))))
map_cost <- function(beta) (sigmab^2)*mle_cost(beta) + 0.5*sum(beta^2)
```

Notice that differently from the methods used above, Logistic Regression requires our data to have a 
constant feature of $1$s in order to fit the bias coefficient. 
```{r train test logistic regression, echo=FALSE}
Xtrain <- as.matrix(cbind(1, training_data))
Xtest <- as.matrix(cbind(1, test_data))
```
We've implemented from scratch Newton's Method for both Maximum Likelihood Estimation
and Maximum-A-Posteriori, with the formulas shown above. In addition, we've used the BFGS implementation 
provided by the R `optim` function. 

```{r MAP logistic regression, echo=FALSE}
# MAP, BFGS
map_bfgs <- logistic_regression(Xtrain, training_classes, cost = "MAP", 
                                method = "BFGS")
yhat_map_bfgs <- predict(map_bfgs, Xtest)
acc_map_bfgs <- calc_accuracy(test_classes, yhat_map_bfgs)
# MAP, NEWTON
map_nm <- logistic_regression(Xtrain, training_classes, cost = "MAP", 
                              method = "NEWTON", niter = 250)
yhat_map_nm <- predict(map_nm, Xtest)
acc_map_nm <- calc_accuracy(test_classes, yhat_map_nm)
```

```{r MLE logistic regression, echo=FALSE}
# MLE, BFGS
mle_bfgs <- logistic_regression(Xtrain, training_classes, cost = "MLE", 
                                method = "BFGS")
yhat_mle_bfgs <- predict(mle_bfgs, Xtest)
acc_mle_bfgs <- calc_accuracy(test_classes, yhat_mle_bfgs)
# MLE, NEWTON
mle_nm <- logistic_regression(Xtrain, training_classes, cost = "MLE", 
                              method = "NEWTON", niter = 250)
yhat_mle_nm <- predict(mle_nm, Xtest)
acc_mle_nm <- calc_accuracy(test_classes, yhat_mle_nm)
```
Below we can see the accuracy of the different optimization methods (BFGS and Newton's method) on the different cost functions (MAP or MLE).
```{r MAP and MLE results logistic regression, echo=FALSE}
# put everything together into a nice table
results_matrix <- matrix(c(
  acc_mle_bfgs, acc_mle_nm,
  acc_map_bfgs, acc_map_nm
),
dimnames = list(c("BFGS", "NM"), c("MLE", "MAP")),
nrow = 2, ncol = 2
)
results <- data.frame(results_matrix)
kable(results)
```

```{r fig.width=7, fit.height=5, fig.align='center',echo=FALSE}
g1 <- confusion_plot(test_classes, yhat_mle_bfgs,'MLE-BFGS')
g2 <- confusion_plot(test_classes, yhat_mle_nm,'MLE-NEWTON')

g3 <- confusion_plot(test_classes, yhat_map_bfgs,'MAP-BFGS')
g4 <- confusion_plot(test_classes, yhat_map_nm,'MAP-NEWTON')
ggarrange(g1,g2,g3,g4,nrow=2,ncol=2)
```


Both MAP and MLE fail to provide us with uncertainty estimates. A fully-Bayesian approach is intractable due to the form of the joint likelihood but we can obtain samples from the posterior using a Random-Walk Metropolis Hastings algorithm. These samples will be parameter vectors defining different decision boundaries. In the following section we sample from the logistic posterior where the dataset has been embedded in two dimensions using PCA. This allows us to plot the decision boundaries given by the samples and thus have an idea of the uncertainty described by the posterior distribution.

### Random-Walk Metropolis-Hastings Implementation on Reduced Data
Similarly as to what we've done with SVMs and Naive Bayes, we can apply Logistic Regression to the data after it has been reduced to two dimensions using PCA. Below we implement a function for the Random-Walk Metropolis-Hastings algorithm and, to make it more efficient, we work with the log posterior, modify the decision rule and pre-calculate all the normal and uniform samples.

\RestyleAlgo{boxruled}
\begin{algorithm}[ht]
  \caption{Metropolis-Hastings}
  Set starting value $\vbeta_0\leftarrow\vbeta_{\text{MAP}}$. \\
  \noindent Sample from standard MVNs $\vect{n}_1, \ldots, \vect{n}_N \sim N(\vzero, H^{-1}(\vbeta_{MAP}))$. \\
  \noindent Sample from Uniform and take the log $u_1', \ldots, u_N'\sim \uniform$ and $u_i := \log(u_i')$.\\
  \noindent \textbf{for} $i=1,2,\ldots,N$ \textbf{do}:\\
  \noindent \qquad Draw a sample from the proposal distribution $\vbeta^*_i \leftarrow \vbeta_i + \vect{n}_i$\\
  \noindent \qquad \textbf{if} $u_i \leq \log p(\vbeta^*_i) - \log p(\vbeta_i)$: \\
  \noindent \qquad \qquad Accept candidate $\vbeta_i \leftarrow \vbeta^*_i$ \\
  \noindent \qquad \textbf{else}:\\
  \noindent \qquad \qquad Reject $\vbeta^*_i$ and use the value of $\vbeta_i$ as the realization of $\vbeta_{i+1}$.\\
  \noindent \qquad \textbf{end}\\
  \noindent \textbf{end}  
\end{algorithm}


```{r RWMH definition}
rwmh_multivariate_log <- function(start, niter, logtarget, vcov, thinning, burnin){
    # Set current z to the initial point and calculate its log target to save computations
    z  <- start; pz <- logtarget(start)
    # create vector deciding iterations where we record the samples
    store <- seq(from=(1+burnin), to=niter, by=thinning)
    # Generate matrix containing samples. Initialize with the starting value
    samples <- matrix(0, nrow=length(store), ncol=nrow(start))
    samples[1, ] <- start
    # Generate uniform random numbers in advance, to save computation. Log them.
    log_u <- log(runif(niter))
    # Proposal is a standard MVN. Generate samples and use linearity later
    vcov <- diag(nrow(start)) %*% vcov
    normal_shift <- mvrnorm(n=niter, mu=c(0,0,0), Sigma=vcov)
    for (i in 2:niter){
        # Sample a candidate and calculate log density there
        candidate <- z + normal_shift[i, ]
        p_candidate <- logtarget(candidate)
        if (log_u[i] <= p_candidate - pz){ # Modify decision rule with bijection
            z  <- candidate; pz <- p_candidate
        }
        # Finally add the sample to our matrix of samples
        if (i %in% store) samples[which(store==i), ] <- z
    }
  return(samples)
}
```

```{r reduced train logistic regression, echo=FALSE}
# need to add a bias feature
Xrwmh <- cbind(1, as.matrix(pca_reduced_training_data[, c(1, 2)]))
Xtest_embedded <- cbind(1, as.matrix(embedded_test_data[, c(1, 2)]))
```

We start our RWMH using the MAP estimate and we use the inverse of the approximated hessian matrix (returned by `optim`) as the variance-covariance matrix for the proposal, so that the algorithm won't waste in regions of the state space that have no mass due to a poor initial guess.

```{r log posterior and optim logistic regression, echo=FALSE}
# up to normalizing constant
log_posterior_unnormalized <- function(beta) {
  log_prior <- -0.5 * sum(beta^2)
  log_likelihood <- -sum(log(1 + exp((1 - 2 * training_classes) * (Xrwmh %*% beta))))
  return(log_prior + log_likelihood)
}
# To start the algorithm more efficiently, start from MAP estimate
optim_results <- optim(c(0, 0, 0), function(x) -log_posterior_unnormalized(x),
                       method = "BFGS", hessian = TRUE)
start <- matrix(optim_results$par)
# Use inverse of approximate hessian matrix as vcov of normal proposal
vcov <- solve(optim_results$hessian)
```

```{r samples logistic regression, echo=FALSE}
samples <- rwmh_multivariate_log(start, niter=50000, log_posterior_unnormalized,
                                 vcov, thinning=1, burnin=0)
```

\newpage

```{r, echo=FALSE}
# MLE, BFGS
mle_bfgs_r <- logistic_regression(Xrwmh, training_classes, cost="MLE", method="BFGS")
acc_mle_bfgs_r <- calc_accuracy(test_classes, predict(mle_bfgs_r, Xtest_embedded))
# MLE, NEWTON
mle_nm_r <- logistic_regression(Xrwmh, training_classes, cost="MLE", method="NEWTON")
acc_mle_nm_r <- calc_accuracy(test_classes, predict(mle_nm_r, Xtest_embedded))
# MAP, BFGS
map_bfgs_r <- logistic_regression(Xrwmh, training_classes, cost="MAP", method="BFGS")
acc_map_bfgs_r <- calc_accuracy(test_classes, predict(map_bfgs_r, Xtest_embedded))
# MAP, NEWTON
map_nm_r <- logistic_regression(Xrwmh, training_classes, cost="MAP", method="NEWTON")
acc_map_nm_r <- calc_accuracy(test_classes, predict(map_nm_r, Xtest_embedded))
# nice table
embedded_results_lr <- matrix(c(
  acc_mle_bfgs_r, acc_mle_nm_r,
  acc_map_bfgs_r, acc_map_nm_r
),
dimnames = list(c("BFGS", "NM"), c("MLE", "MAP")),
nrow = 2, ncol = 2
)
embedded_results_lr <- data.frame(embedded_results_lr)
kable(embedded_results_lr)
```

```{r fig.width=9, fit.height=5, fig.align='center', echo=FALSE}
g1 <- confusion_plot(test_classes, predict(mle_bfgs_r, Xtest_embedded),'MLE-BFGS')
g2 <- confusion_plot(test_classes, predict(mle_nm_r, Xtest_embedded),'MLE-NEWTON')

g3 <- confusion_plot(test_classes, predict(map_bfgs_r, Xtest_embedded),'MAP-BFGS')
g4 <- confusion_plot(test_classes, predict(map_nm_r, Xtest_embedded),'MAP-NEWTON')
ggarrange(g1,g2,g3,g4,nrow=2,ncol=2)
```

The histograms of each coordinate of the samples show RWMH is sampling sensibly.

```{r, echo=FALSE}
samplesdf <- data.frame(samples) %>% mutate(iterations = row_number())
```
```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=8, eval=FALSE}
trace1 <- ggplot(data = samplesdf, aes(x = iterations, y = X1)) + geom_line()
trace2 <- ggplot(data = samplesdf, aes(x = iterations, y = X2)) + geom_line()
trace3 <- ggplot(data = samplesdf, aes(x = iterations, y = X3)) + geom_line()
grid.arrange(trace1, trace2, trace3, ncol = 1)
```
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align="center"}
hist1 <- ggplot(data = samplesdf, aes(x = X1, stat(density))) +
  geom_histogram(binwidth = 0.05, alpha = 0.5, fill = "turquoise1", color = "turquoise4") + 
  labs(x=TeX('$\\beta_0$'))
hist2 <- ggplot(data = samplesdf, aes(x = X2, stat(density))) +
  geom_histogram(binwidth = 0.02, alpha = 0.5, fill = "turquoise1", color = "turquoise4") +
  labs(x=TeX('$\\beta_1$'))
hist3 <- ggplot(data = samplesdf, aes(x = X3, stat(density))) +
  geom_histogram(binwidth = 0.02, alpha = 0.5, fill = "turquoise1", color = "turquoise4") +
  labs(x=TeX('$\\beta_2$'))
grid.arrange(hist1, hist2, hist3, ncol = 1)
```

The next thing to do is to plot the decision boundary given by $\vbeta_{\text{MAP}}$ and the decision boundaries given by (some) of the samples from the posterior. 

```{r, echo=FALSE, fig.align="center", fig.height=3, fig.width=6}
samples_to_select <- 200
samples_subset <- samples[sample(1:nrow(samples), samples_to_select), ]
# Calculate slope and intercept for those samples. Then calculate x2 from x1
linecoefs <- cbind(-samples_subset[, 2] / samples_subset[, 3], -samples_subset[, 1] / samples_subset[, 3])
x2_vals <- apply(linecoefs, 1, function(row) Xrwmh[, 2] * row[1] + row[2])
# Store x2 values together with x1 values from X. Then melt to plot all lines
dfsample_lines <- data.frame(x1 = Xrwmh[, 2], x2_vals) %>%
  gather("key", "value", -x1)
# replace diagnosis
data_plot <- pca_reduced_training_data %>% mutate(training_classes = training_classes)
# Create dataframe containing values for the MAP line
dfmap <- data.frame(x1 = Xrwmh[, 2], y = (-start[2, ] / start[3, ]) * Xrwmh[, 2] + (-start[1, ] / start[3, ]))
ggplot() +
  geom_point(data = data_plot, aes(x = X1, y = X2, color = as.factor(training_classes))) + # Dataset scatter plot
  coord_cartesian(xlim = c(-550, -300), ylim = c(-550, -300)) +
  geom_line(data = dfsample_lines, aes(x = x1, y = value, group = key), alpha = 0.1, color = "grey50") +
  geom_line(data = dfmap, aes(x = x1, y = y), color = "black") + # MAP line
  labs(color = TeX("Class"), x=TeX("Principal Component 1"), y=TeX("Principal Component 2")) +
  theme(plot.title = element_text(hjust = 0.5, size = 20))
```

We can see that Logistic Regression finds a decision boundary that does not reflect what we would expect it to be. In particular, we can see that the points seem to lie on a line with very little variance along the perpendicular direction and since, differently from SVMs, Logistic Regression has no concept of a margin, it settles for a boundary that would easily allow points to be misclassified if they were perturbed along the perpendicular direction. This is also reflected in the confidence of the posterior distribution, having samples which give very similar decision boundaries.

\newpage
# Conclusion

```{r, fig.align="center", echo=FALSE}
# merge all predictions
id <- seq(length(id_test))
all_pred <- cbind(id, numeric_test_labels, pred_naive, pred_naive_pca,pred_svm,yhat_map_bfgs,yhat_map_nm,yhat_mle_bfgs,yhat_mle_nm)
all_pred[all_pred == -1] <- 0
all_pred %<>% as.data.frame()
colnames(all_pred) <- c('id','actual','naive','naive_pca','SVM','map_bfgs','map_nm','mle_bfgs','mle_nm')
```


Visualize which are the observations that the models missclassify.
```{r, fig.align="center", echo=FALSE, fig.cap='Plot of misclassified observations for each method.'}
all_pred %>% 
  mutate_at(vars(-c(id,actual)),funs(.==actual)) %>% 
  dplyr::select(-'actual') %>% 
  melt(id='id') %>%
  ggplot(aes(x = id, y = variable, fill = value)) +
  geom_raster() +
  theme_minimal()
```



