---
title: "SC1_Proj"
author: "Alessio"
date: "13 January 2020"
output: pdf_document
---

```{r, include=FALSE}
library(ggplot2)
library(magrittr)
library(dplyr)
library(corrplot)
library(tidyr)
library(readr)
library(devtools)
library(cowplot)
#library(tsne)
```

#Description of the dataset and problem
TODO: describe

```{r}
data <- read_csv("../data/data.csv")
glimpse(data)
colnames(data)[3:32] <- c('radius_m','texture_m', 'perim_m','area_m','smooth_m','compact_m','concav_m','concav_pt_m','symmetry_m','frac_dim_m','radius_se','texture_se', 'perim_se','area_se','smooth_se','compact_se','concav_se','concav_pt_se','symmetry_se','frac_dim_se','radius_w','texture_w', 'perim_w','area_w','smooth_w','compact_w','concav_w','concav_pt_w','symmetry_w','frac_dim_w')
```

#Dataset Preprocessing Visualisation and Exploration

```{r}
colSums(is.na(data))
```


```{r}
data %<>% mutate_at(vars(diagnosis), factor)
```

```{r}
train <- data %>% sample_frac(0.8)
test <- anti_join(data,train, by='id')
```

```{r}
data %<>%  
  dplyr::select(-c(id, X33))
train %<>%  
  dplyr::select(-c(id, X33))
test %<>%  
  dplyr::select(-c(id, X33))
```

```{r}
sum(is.na(data))
```

```{r}
training_data <- train[2:dim(train)[2]] 
training_classes <- train[1] 

test_data <- test[2:dim(test)[2]] 
test_classes <- test[1] 
```

```{r echo=FALSE}
corr <- training_data[,-1] %>% 
          cor() %>% 
           round(1)

corrplot(corr, method='circle',order='hclust',tl.col = "black")
```

```{r fig.height=40, fig.width=30, echo=FALSE}
train %>% 
  group_by(diagnosis) %>% 
  gather(key="var",value="value",-diagnosis) %>% 
  ggplot(aes(x=value))+
  facet_wrap(~var,scales='free',nrow=6)+
  geom_density(aes(fill=diagnosis),alpha=0.4)+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position="bottom")
```

```{r echo=FALSE}
g1<-ggplot(training_classes,aes(x=diagnosis,fill=diagnosis))+
  geom_bar(aes(y=(..count..)/sum(..count..)))+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position='none')
  

g2<-ggplot(test_classes,aes(x=diagnosis,fill=diagnosis))+
  geom_bar(aes(y=(..count..)/sum(..count..)))+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position='none')
plot_grid(g1,g2)
```


#Dimensionality Reduction and Feature Selection
  - PCA
Code
```{r}
normalise_z <-  function(X){
  mean_cols <- colMeans(X)
  sd_cols <- apply(X, 2, sd)
  mean_normalised_X <- t(apply(X, 1, function(x){x - mean_cols}))
  normalised_X <- t(apply(mean_normalised_X, 1, function(x){x / sd_cols}))
  return(normalised_X)
}


pca <- function(X, number_components_keep) {
  normalised_X <- normalise_z(X)

  corr_mat <- t(normalised_X) %*% normalised_X

  eigenvectors <- eigen(corr_mat, symmetric=TRUE)$vectors

  reduced_data <-  X %*% eigenvectors[,1:number_components_keep]
  relevant_eigs <-  eigenvectors[,1:number_components_keep]
  returnds <- list(reduced_data, relevant_eigs)
  names(returnds) <- c("reduced_data", "reduction_matrix")
  return(returnds)
}
```

Apply to dataset
```{r}
pca_result <- pca(as.matrix(training_data), 2)
reduced_training_data <- data.frame(cbind(pca_result$reduced_data, training_classes))

ggplot(data=reduced_training_data, aes(x=X1, y=X2)) + geom_point(aes(colour=diagnosis))
```

 tSNE
TODO: try different perplexity parameters
```{r}
reduced_training_data <- tsne::tsne(training_data)

reduced_training_data <- data.frame(cbind(reduced_training_data, training_classes))
ggplot(data=reduced_training_data, aes(x=X1, y=X2)) + geom_point(aes(colour=diagnosis))
```

  - Correlation Feature Selection
  - LDA

# Classification

TODO: Mathematical description 
 
TODO: Basic Code describing implementation  
  
```{r}
soft_margin_svm_plotter <- function(w, b) {
  plotter <- function(x) {
    return(1/w[2]   * -(b + (w[1]*as.numeric(x))))
  }
  return(plotter)
}
```

```{r}
factor_to_label <- function(x)  {
  if(as.character(x) == "M")  {
    return(1)
  }
  else {
    return(-1)
  }
}
```

```{r}
label_to_factor <- function(x) {
  if(x == 1) {
    return(as.factor("M"))
  }
  else{
    return(as.factor("B"))
  }
}
numeric_test_labels <- apply(test_classes, 1, factor_to_label)
numeric_training_labels <- apply(training_classes, 1, factor_to_label)
```


Use PCA then do SVM 
```{r}
model <- svm(X=pca_result$reduced_data, 
             classes=numeric_training_labels, 
             C=100000, margin_type='soft', 
             kernel_function = linear_kernel, 
             feature_map = linear_basis_function)


reduced_prediction_fn <- model$prediction_function

pca_reduced_prediction_fn <- function(x) {
  p <- x %*% pca_result$reduction_matrix
  reduced_prediction_fn(t(p))
}
```
 
```{r}
predictions <- apply(as.matrix(test_data),1, pca_reduced_prediction_fn)
accuracy_calc(numeric_test_labels, predictions)
```

```{r}
svm_plotter <- soft_margin_svm_plotter(model$params$w, model$params$b)
embedded_test_data <- data.frame(cbind(as.matrix(test_data) %*% pca_result$reduction_matrix), test_classes)

ggplot(embedded_test_data, aes(x=as.numeric(X1), y=as.numeric(X2))) + 
  geom_point(aes(colour=
                   test_classes)) + 
  stat_function(fun=svm_plotter)
```

TODO: analyse results
  
  - Naive Bayes
  - SVM
  - Logistic Regression
  - Lasso

#Conclusion
  - Evaluation of results
  - Discuss outliers
  
TODO: create outlier plot