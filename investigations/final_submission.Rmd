---
title: "SC1_Proj"
author: "Alessio"
date: "13 January 2020"
output: pdf_document
---

```{r, include=FALSE}
library(ggplot2)
library(magrittr)
library(dplyr)
library(ggcorrplot)
library(tidyr)
library(readr)
library(devtools)
library(tsne)
library(devtools)
install_github("alessio-b-zak/sc1-svm-example-package")
library(alessiosvm)
```

#Description of the dataset and problem
TODO: describe

```{r}
data <- read_csv("../data/data.csv")
```

#Dataset Preprocessing Visualisation and Exploration

```{r}
colSums(is.na(data))
```


```{r}
data %<>% mutate_at(vars(diagnosis), factor)
```

```{r}
train <- data %>% sample_frac(0.8)
test <- anti_join(data,train, by='id')
```

```{r}
data %<>%  
  dplyr::select(-c(id, X33))
train %<>%  
  dplyr::select(-c(id, X33))
test %<>%  
  dplyr::select(-c(id, X33))
```

```{r}
training_data <- train[2:dim(train)[2]] 
training_classes <- train[1] 

test_data <- test[2:dim(test)[2]] 
test_classes <- test[1] 
```


```{r}
sum(is.na(data))
```

```{r}
corr <- data[,-1] %>% 
          cor() %>% 
           round(1)
ggcorrplot(corr,
           hc.order = TRUE,
           colors = c("#6D9EC1", "white", "#E46726"),
           ggtheme = ggplot2::theme_minimal)
```

  - Class Frequencies
  - Density
  - box plots




#Dimensionality Reduction and Feature Selection
  - PCA
Code
```{r}
normalise_z <-  function(X){
  mean_cols <- colMeans(X)
  sd_cols <- apply(X, 2, sd)
  mean_normalised_X <- t(apply(X, 1, function(x){x - mean_cols}))
  normalised_X <- t(apply(mean_normalised_X, 1, function(x){x / sd_cols}))
  return(normalised_X)
}


pca <- function(X, number_components_keep) {
  normalised_X <- normalise_z(X)
  
  corr_mat <- t(normalised_X) %*% normalised_X
  
  eigenvectors <- eigen(corr_mat, symmetric=TRUE)$vectors
  
  reduced_data <-  X %*% eigenvectors[,1:number_components_keep]
  relevant_eigs <-  eigenvectors[,1:number_components_keep]
  returnds <- list(reduced_data, relevant_eigs)
  names(returnds) <- c("reduced_data", "reduction_matrix")
  return(returnds)
}
``` 

Apply to dataset
```{r}
pca_result <- pca(as.matrix(training_data), 2)
pca_reduced_training_data <- data.frame(cbind(pca_result$reduced_data, training_classes))

ggplot(data=pca_reduced_training_data, aes(x=X1, y=X2)) + geom_point(aes(colour=diagnosis))
```
  
 tSNE 
TODO: try different perplexity parameters
```{r}
reduced_training_data <- tsne::tsne(training_data)

reduced_training_data <- data.frame(cbind(reduced_training_data, training_classes))
ggplot(data=reduced_training_data, aes(x=X1, y=X2)) + geom_point(aes(colour=diagnosis))
```

  - Correlation Feature Selection
  - LDA

# Classification
TODO: Mathematical description 
 
TODO: Basic Code describing implementation  
  
```{r}
soft_margin_svm_plotter <- function(w, b) {
  plotter <- function(x) {
    return(1/w[2]   * -(b + (w[1]*as.numeric(x))))
  }
  return(plotter)
}
```

```{r}
factor_to_label <- function(x)  {
  if(as.character(x) == "M")  {
    return(1)
  }
  else {
    return(-1)
  }
}
```

```{r}
label_to_factor <- function(x) {
  if(x == 1) {
    return(as.factor("M"))
  }
  else{
    return(as.factor("B"))
  }
}
numeric_test_labels <- apply(test_classes, 1, factor_to_label)
```


Use PCA then do SVM 
```{r}
model <- svm(X=pca_result$reduced_data, 
             classes=numeric_training_labels, 
             C=100000, margin_type='soft', 
             kernel_function = linear_kernel, 
             feature_map = linear_basis_function)


reduced_prediction_fn <- model$prediction_function

pca_reduced_prediction_fn <- function(x) {
  p <- x %*% pca_result$reduction_matrix
  reduced_prediction_fn(t(p))
}
```
 
```{r}
predictions <- apply(as.matrix(test_data),1, pca_reduced_prediction_fn)
accuracy_calc(numeric_test_labels, predictions)
```
TODO: rename axis
```{r}
svm_plotter <- soft_margin_svm_plotter(model$params$w, model$params$b)
embedded_test_data <- data.frame(cbind(as.matrix(test_data) %*% pca_result$reduction_matrix), test_classes)

ggplot(embedded_test_data, aes(x=as.numeric(X1), y=as.numeric(X2))) + 
  geom_point(aes(colour=diagnosis)) + 
  stat_function(fun=svm_plotter)
```

TODO: analyse results
  
  - Naive Bayes
  - Logistic Regression
  - Lasso

#Conclusion
  - Evaluation of results
  - Discuss outliers