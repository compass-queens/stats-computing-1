---
title: "SC1_Proj"
author: "Alessio"
date: "13 January 2020"
output: pdf_document
urlcolor: blue
header-includes:
- \usepackage{amsmath}
- \usepackage{bm}
---
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\vbeta}{\vect{\beta}}
\newcommand{\vx}{\vect{x}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vzero}{\vect{0}}
\newcommand{\vSigma}{\vect{\Sigma}}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
```

```{r, include=FALSE}
library(ggplot2)
library(magrittr)
library(dplyr)
library(corrplot)
library(tidyr)
library(readr)
library(devtools)
library(tsne)
library(devtools)
library(ggpubr)
library(reshape2)
library(gridExtra)
library(knitr)
install_github("alessio-b-zak/sc1-svm-example-package")
install_github("MauroCE/LogisticRegression")
install_github("andreabecsek/NaiveBayes")
library(alessiosvm)
library(LogisticRegression)
library(NaiveBayes)
```

```{r  echo=FALSE}
theme_set(theme_minimal())
```

# Introduction
One of the most common types of cancer diagnosed in women is breast cancer. There are multiple tests that people are subjected to, but one of the most indicative ones is fine needle aspiration which involes extracting a sample of cells to be examined under a microscope. Multiple numerical metrics are computed from the obtained images. The aim is to use the extracted metrics to make accurate diagnoses.

The dataset consists of $569$ images which have been processed as described and a total of $30$ variables have been computed for each observation.

The aim of this report is to implement a number of classification algorithms, use them to obtain predictions, and compare their performances.


# Exploratory analysis


```{r}
data <- read_csv("../data/data.csv")
colnames(data)[3:32] <- c('radius_m','texture_m', 'perim_m','area_m','smooth_m','compact_m',
                          'concav_m','concav_pt_m','symmetry_m','frac_dim_m','radius_se',
                          'texture_se','perim_se','area_se','smooth_se','compact_se',
                          'concav_se','concav_pt_se','symmetry_se','frac_dim_se','radius_w',
                          'texture_w','perim_w','area_w','smooth_w','compact_w','concav_w',
                          'concav_pt_w','symmetry_w','frac_dim_w')
```

Check for missing values in every column.
```{r}
colSums(is.na(data))
```

```{r}
data %<>% mutate_at(vars(diagnosis), factor)
```

```{r}
train <- data %>% sample_frac(0.8)
test <- anti_join(data,train, by='id')

# need ids for later
id_train <- train$id
id_test <- test$id
```

```{r}
data %<>%  
  dplyr::select(-c(id, X33))
train %<>%  
  dplyr::select(-c(id, X33))
test %<>%  
  dplyr::select(-c(id, X33))
```

```{r}
sum(is.na(data))
```

```{r}
training_data <- train[2:dim(train)[2]] 
training_classes <- train[1] 
test_data <- test[2:dim(test)[2]] 
test_classes <- test[1] 
```

```{r fig.height=40, fig.width=30, echo=FALSE}
train %>% 
  group_by(diagnosis) %>% 
  gather(key="var",value="value",-diagnosis) %>% 
  ggplot(aes(x=value))+
  facet_wrap(~var,scales='free',nrow=6)+
  geom_density(aes(fill=diagnosis),alpha=0.4)+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position="bottom")
```

```{r echo=FALSE}
g1<-ggplot(training_classes,aes(x=diagnosis,fill=diagnosis))+
  geom_bar(aes(y=(..count..)/sum(..count..)))+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position='none')
  

g2<-ggplot(test_classes,aes(x=diagnosis,fill=diagnosis))+
  geom_bar(aes(y=(..count..)/sum(..count..)))+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position='none')
ggarrange(g1,g2)
```

```{r echo=FALSE}
corr <- training_data[,-1] %>% 
          cor() %>% 
           round(1)

corrplot(corr, method='circle',order='hclust',tl.col = "black")
```

\newpage
# Dimensionality Reduction and Feature Selection

## PCA

For data of a non-trivial dimensionality it can be difficult to know where to begin with the modelling process as we may not have an intuitive idea of the underlying structure in our code. One such method of visualising the data in a lower space is principal component analysis (PCA). PCA aims to produce a set of linearly uncorrelated variables from our original set of variables of a reduced size. It does this by first taking the dataset represented as a matrix:

$$
X = \begin{pmatrix}
x_{1}^{T} \\
\vdots \\
x_{2}^{T}
\end{pmatrix}
$$
then we form a matrix normalised by the standard score by subtracting the column means from every column and dividing every column by the standard deviation for that column as in:
```{r}
normalise_z <-  function(X){
  mean_cols <- colMeans(X)
  sd_cols <- apply(X, 2, sd)
  mean_normalised_X <- t(apply(X, 1, function(x){x - mean_cols}))
  normalised_X <- t(apply(mean_normalised_X, 1, function(x){x / sd_cols}))
  return(normalised_X)
}
```
Giving us:
$$
Z = \begin{pmatrix} 
\frac{x_{11} - \mu_{1}}{\sigma_{1}} & ... & \frac{x_{1d} - \mu_{d}}{\sigma{d}} \\
\vdots & \vdots & \vdots \\
\frac{x_{n1} - \mu_{1}}{\sigma_{1}} & ... & \frac{x_{nd} - \mu_{d}}{\sigma{d}} \\
\end{pmatrix}
$$
Multiplying this by its transpose gives us the correlation matrix where the entry $\rho_{ij}$ is the correlation between observation $i$ and observation $j$. We can take the eigendecomposition of this matrix product to give us:

$$
Z^{T}Z = P \,\Sigma^{-1}P^{T}
$$
Where we assume that the diagonal $\Sigma$ is ordered by size. The eigenvectors corresponding to the largest eigenvalues represent the combinations of features which account for the highest variance. If we wish to visualise at dimension $k < d$ we can simply take the top $k$ eigenvectors as a matrix and multiply our data by this visualise our data in the reduced space. Code that does this can be found below.
```{r}
pca <- function(X, number_components_keep) {
  normalised_X <- normalise_z(X)

  corr_mat <- t(normalised_X) %*% normalised_X

  eigenvectors <- eigen(corr_mat, symmetric=TRUE)$vectors

  reduced_data <-  X %*% eigenvectors[,1:number_components_keep]
  relevant_eigs <-  eigenvectors[,1:number_components_keep]
  returnds <- list(reduced_data, relevant_eigs)
  names(returnds) <- c("reduced_data", "reduction_matrix")
  return(returnds)
}
```

When applying this to the dataset and keeping 2 components the following plot results:
```{r, echo=FALSE}
pca_result <- pca(as.matrix(training_data), 2)
pca_reduced_training_data <- data.frame(cbind(pca_result$reduced_data, training_classes))

ggplot(data=pca_reduced_training_data, aes(x=X1, y=X2)) + geom_point(aes(colour=diagnosis))
```
  
<!-- # tSNE  -->
<!-- #TODO: try different perplexity parameters -->
<!-- #```{r} -->
<!-- #reduced_training_data <- tsne::tsne(training_data) -->
<!-- # -->
<!-- #reduced_training_data <- data.frame(cbind(reduced_training_data, training_classes)) -->
<!-- #ggplot(data=reduced_training_data, aes(x=X1, y=X2)) + geom_point(aes(colour=diagnosis)) -->
<!-- #``` -->

  - Correlation Feature Selection
  - LDA
  
\newpage

# Classification

add list of methods+measure 

## SVM
From the application of PCA to the dataset we can see that, after reducing to 2 dimensions, the data appears to be almost linearly separable. Given this, an appropriate method of classifying the data would be to apply a soft-margin SVM to the reduced dimension data. Soft-margin SVMs solve the problem of classifying non-separable data by permitting a certain number of points to be incorrectly classifed however the number and the amount they violate the constraints by must be as small as possible. After manipulating the reformulated optimisation problem we end up with the optimisation problem
$$
\underset{\lambda}{\text{min}} \, \frac{\overline{\lambda}X X^{T}\overline{\lambda^{T}}}{4}
+ \lambda^{T}\bm{1}
$$
$$
\text{such that } 0 \leq \lambda_{i} \leq C
$$
$$
\text{and } \sum\limits_{i}^{n} \lambda_{i}y_{i} = 0
$$

where
$$
X = \begin{pmatrix}
x_{1}^{T} \\
\vdots \\
x_{n}^{T}
\end{pmatrix} \text{ and } 
\overline{\lambda} = [\lambda_{1} \cdot y_{i}, ... , \lambda_{n} \cdot y_{n}] \text{ and } \bm{1} = [1, ..., 1] \in \mathbb{R^{n}}
$$
For some predefined $C$. This is a quadratic programming problem with linear constraints which can be solved using the R package `quadprog`with the function `solve.QP`. From its documentation, this function can solve (for $b$) problems in the form $\underset{b}{\text{min}}(-d^{T}b + \frac{1}{2} b^{T}Db)$ with the constraints that $A^{T} b \geq b_0$. By transforming the above problem into this format we can implement soft-margin SVM using the following code
```{r, eval=FALSE}
train_soft_svm <- function(X, y, C) {
  
  num_observation <- nrow(X)
  dim_num <- ncol(X)

  Dmat2 <- diag(y) * X %*% t(X) %*% diag(y)
  diag(Dmat2) <- diag(Dmat2) + 1e-6
  dv2 <- rep(1, num_observation)
  
  A2 <- rbind( y,diag(num_observation)) 
  A2 <- rbind(A2, -1*diag(num_observation))
  
  bv2 <- c(c(0), rep(0, num_observation), rep(-C, num_observation) )
  model <- solve.QP(Dmat2, dv2, t(A2), bv2, meq = 1) 
}
```

In order to recover $w$ and $b$ from $\lambda$ we use the relationship
$$
w = \sum\limits_{i=0}^{n-1}\lambda_{i}x_{i}^{T}y_{i}
$$
and
$$
b = \text{mean}(\sum\limits_{i=0}^{k} y_{i} - w \cdot x_{i}) \, . \, \forall \, i . 0 < \lambda_{i} < C
$$

Which can be made as functions in R as so:

```{r, eval=FALSE}
calculate_b <- function(w, X, y, a, C) {
  ks <- sapply(a, function(x){return(x > 0 && x < C)})
  indices <- which(ks)
  sum_bs <- 0
  for(i in indices) {
    sum_bs <- sum_bs + (y[i] - w %*% X[i,]) 
  }
  return(sum_bs / length(indices))
}


recover_w <- function(a, y, X){
  colSums(diag(a) %*% diag(y) %*% X)
}
```


From the parameters we can recover the equation of the line corresopnding the decision boundary which can later be used for plotting
```{r}
soft_margin_svm_plotter <- function(w, b) {
  plotter <- function(x) {
    return(1/w[2]   * -(b + (w[1]*as.numeric(x))))
  }
  return(plotter)
}
```

```{r, echo=FALSE}
factor_to_label <- function(x)  {
  if(as.character(x) == "M")  {
    return(1)
  }
  else {
    
    return(-1)
  }
}
```

```{r, echo=FALSE}
label_to_factor <- function(x) {
  if(x == 1) {
    return(as.factor("M"))
  }
  else{
    return(as.factor("B"))
  }
}
numeric_test_labels <- apply(test_classes, 1, factor_to_label)
numeric_training_labels <- apply(training_classes, 1, factor_to_label)
```

TODO: prediction function

We are now able to define a function which first embeds the vector in the reduced space and then predicts the class based upon the prediction function produced by the parameters found by the SVM. Below is code that will do the following based upon a package written for this assignment found [here](https://github.com/alessio-b-zak/sc1-svm-example-package)
```{r}
model <- svm(X=pca_result$reduced_data, 
             classes=numeric_training_labels, 
             C=100000, margin_type='soft', 
             kernel_function = linear_kernel, 
             feature_map = linear_basis_function)


reduced_prediction_fn <- model$prediction_function

pca_reduced_prediction_fn <- function(x) {
  p <- x %*% pca_result$reduction_matrix
  reduced_prediction_fn(t(p))
}
```
 
```{r, echo=FALSE}
pred_svm <- apply(as.matrix(test_data),1, pca_reduced_prediction_fn)
```

When running the trained SVM prediction functio on the test set we achieve `r accuracy_calc(numeric_test_labels, pred_svm)` %. We can plot this code as below

```{r, echo=FALSE}
svm_plotter <- soft_margin_svm_plotter(model$params$w, model$params$b)
embedded_test_data <- data.frame(cbind(as.matrix(test_data) %*% pca_result$reduction_matrix), test_classes)

ggplot(embedded_test_data, aes(x=as.numeric(X1), y=as.numeric(X2))) + 
  geom_point(aes(colour=diagnosis)) + 
  stat_function(fun=svm_plotter)
```

\newpage
# Naive Bayes
## Mathematical setting

Let $y$ be the class label that we want to assign to an observation $\boldsymbol{x}=(x_1,\cdots x_d)$, where $x_1,\cdots x_d$ are the features. The probability of an observation having label $y$ is given by Bayes rule,
\begin{align*}
P(y|x_1,\cdots,x_d)&=\frac{P(x_1,\cdots,x_d|y_k)P(y)}{P(x_1,\cdots,x_d)}\\
&\propto P(x_1,\cdots,x_d|y_k)P(y).
\end{align*}

The prior class probability $P(y)$ can be easily obtained by the proportion of observation that are in the given class.


The main assumption is that every feature is conditionally independent given the class label $y$. The reason why this classifier is called *naive* is that very often this assumption is not actually realistic.


This assumption simplifies the posterior to
$$P(y|x_1,\cdots,x_d) \propto P(y)\prod_{i=1}^d P(x_i|y).$$
There are various types of Naive Bayes classifiers based on the type of features. In our case, since we have continuous variables we assume that all features are normally distributed. Therefore, the conditional probabilities can be calculated as
$$P(x_i|y)=\frac{1}{\sqrt{2\pi\sigma_y^2}}exp\left(-\frac{(x_i-\mu_y)^2}{2\sigma_y^2}\right)$$

Finally, to assign the class to an observation we use the Maximum A Posteriori decision rule. For every observation, we pick the class the has the highest probability
$$y=\underset{y}{\text{argmax}}P(y)\prod_{i=1}^dP(x_i|y).$$

## Implementation

*Here are some code snippets just to illustrate how these theoretical aspects are implemented. The full code can be found in the package.*

The observations are stored as rows in $X$ and the corresponding class labels are entires in the column matrix $y$.

First we calculate the prior class probabilities based on the number of observations in each class.

```{r eval = FALSE}
n <- dim(X)[1]
d <- dim(X)[2]
classes <- sort(unique(y)[, 1])
k <- length(classes)

prior <- rep(0, k)
for (i in 1:k) {
  prior[i] <- sum(y == classes[i]) / n
}
```

Then we create an array of the mean and sd of the data split by clasess and features.

```{r eval = FALSE}
summaries <- array(rep(1, d * k * 2), dim = c(k, d, 2))
for (i in 1:k) {
  X_k <- X[which(y == (i - 1)), ]
  summaries[i, , 1] <- apply(X_k, 2, mean)
  summaries[i, , 2] <- apply(X_k, 2, sd)
}
```

Finally, the predictions are obtained by taking the largest posterior class probability. Note that in order to avoid underflow, we take the maximum of the *log* posterior class probabilities. 

```{r eval = FALSE}
probs <- matrix(rep(0, n * k), nrow = n)
for (obs in 1:n) {
  for (class in 1:k) {
    class_prob <- log(prior[class])
    for (feat in 1:d) {
      mu <- summaries[class, feat, 1]
      sd <- summaries[class, feat, 2]
      cond <- dnorm(x_new[obs, feat], mu, sd, log = TRUE)
      class_prob <- class_prob + cond
      }
     probs[obs, class] <- class_prob
  }
}

pred <- apply(probs, 1, which.max)
```

## Fit model to dataset

```{r}
levels(training_classes$diagnosis) <- c(0,1)
training_classes %<>% as.matrix
mode(training_classes) <- 'numeric'

levels(test_classes$diagnosis) <-c(0,1)
test_classes %<>% as.matrix
mode(test_classes) <- 'numeric'
```

Fit the Naive Bayes model to the data, calculate predictions and check the accuracy using.
```{r}
model_naive <- naive_bayes(training_data,training_classes)

pred_naive <- predict(model_naive,as.matrix(test_data))

# confusion_plot(test_classes,pred_naive)

calc_accuracy <- function(ytest, yhat) sum(drop(yhat) == drop(ytest)) / length(drop(ytest))

acc_naive <- calc_accuracy(test_classes,pred_naive)
acc_naive
```

\newpage

# Logistic Regression
## Mathematical Setting
Let $Y_i\mid \vx_i \sim \text{Bernoulli}(p_i)$ with $p_i = \sigma(\vx_i^\top \vbeta)$ where $\sigma(\cdot)$ is the **sigmoid function**. The joint log-likelihood is given by
$$
\ln p(\vy\mid \vbeta) = \sum_{i=1}^n y_i \ln(p_i) + (1 - y_i)\ln(1 - p_i)=-\sum_{i=1}^n\ln\left(1 + \exp((1 - 2y_i)\vx_i^\top\vbeta)\right)
$$

### Maximum Likelihood Estimation
Maximizing the likelihood is equivalent to minimizing the negative log-likelihood. Minimizing the negative log likelihood is equivalent to solving the following optimization problem

$$
\min_{\vbeta}\sum_{i=1}^n\ln\left(1 + \exp((1 - 2y_i)\vx_i^\top\vbeta)\right)
$$

### Maximum-A-Posteriori and Ridge Regularization
We can introduce an isotropic Gaussian prior on **all** the coefficients $p(\vbeta) = N(\vzero, \sigma_{\vbeta}^2 I)$. Maximizing the posterior $p(\vbeta \mid \vy)$ is equivalent to minimizing the negative log posterior $-\ln p(\vbeta\mid \vy)$ giving

$$
\min_{\vbeta} \sigma^2_{\vbeta}\sum_{i=1}^n\ln\left(1 + \exp((1 - 2y_i)\vx_i^\top\vbeta)\right) + \frac{1}{2}\vbeta^\top\vbeta
$$

### Gradient Ascent for Maximum Likelihood Estimation
Updates take the form
$$
\vbeta_{k+1} \leftarrow \vbeta_k + \gamma X^\top(\vy - \sigma(X\vbeta_k))
$$

### Gradient Ascent for Maximum-A-Posteriori
The update takes the form
$$
\vbeta_{k+1}\leftarrow  \vbeta_k + \gamma_k\left[\sigma_{\vbeta}^2X^\top(\vy - \sigma(X\vbeta_k)) - \vbeta_k\right]
$$

### Newton's Method for Maximum Likelihood Estimation
The iterations are as follows, where for stability one can add a learning rate $\alpha$, which is in practice often set to $\alpha=0.1$.
$$
\vbeta_{k+1} \leftarrow \vbeta_k +  \alpha(X^\top D X)^{-1} X^\top(\vy - \sigma(X\vbeta_k))
$$
In practice we would solve the corresponding system for $\vect{d}$
$$
(X^\top D X)\vect{d}_k = \alpha X^\top(\vy - \sigma(X\vbeta_k))
$$
and then perform the update
$$
\vbeta_{k+1}\leftarrow \vbeta_k + \vect{d}_k
$$

### Newton's Method for Maximum-A-Posteriori
The update takes the form
$$
\vbeta_{k+1} \leftarrow \vbeta_k + \alpha \left[\sigma^2_{\vbeta} X^\top D X + I\right]^{-1}\left(\sigma^2_{\vbeta} X^\top (\vy - \sigma(X\vbeta_k)) - \vbeta_k\right)
$$

## Implementation
First, we need to add a column of $1$s to the design matrix so that we can fit the bias coefficient.
```{r train test logistic regression}
Xtrain <- as.matrix(cbind(1, training_data))
Xtest  <- as.matrix(cbind(1, test_data))
```

### Maximum a Posteriori
We implement BFGS, Newton's Method and Gradient Ascent.
```{r MAP logistic regression}
# MAP, BFGS
map_bfgs <- logistic_regression(Xtrain, training_classes, cost="MAP", method="BFGS")
yhat_map_bfgs <- predict(map_bfgs, Xtest)
acc_map_bfgs <- calc_accuracy(test_classes, yhat_map_bfgs)
# MAP, NEWTON
map_nm <- logistic_regression(Xtrain, training_classes, cost="MAP", method="NEWTON", niter=250)
yhat_map_nm <- predict(map_nm, Xtest)
acc_map_nm <- calc_accuracy(test_classes, yhat_map_nm)
# MAP, GRADIENT ASCENT
map_ga <- logistic_regression(Xtrain, training_classes, cost="MAP", method="GA", niter=1000)
yhat_map_ga <- predict(map_ga, Xtest)
acc_map_ga <- calc_accuracy(test_classes, yhat_map_ga)
```

### Maximum Likelihood Estimation
Similarly, we also implement the same algorithms for Maximum Likelihood Estimation.
```{r MLE logistic regression}
# MLE, BFGS
mle_bfgs <- logistic_regression(Xtrain, training_classes, cost="MLE", method="BFGS")
yhat_mle_bfgs <- predict(mle_bfgs, Xtest)
acc_mle_bfgs <- calc_accuracy(test_classes, yhat_mle_bfgs)
# MLE, NEWTON
mle_nm <- logistic_regression(Xtrain, training_classes, cost="MLE", method="NEWTON", niter=250)
yhat_mle_nm <- predict(mle_nm, Xtest)
acc_mle_nm <- calc_accuracy(test_classes, yhat_mle_nm)
# MLE, GRADIENT ASCENT
mle_ga <- logistic_regression(Xtrain, training_classes, cost="MLE", method="GA", niter=1000)
yhat_mle_ga <- predict(mle_ga, Xtest)
acc_mle_ga <- calc_accuracy(test_classes, yhat_mle_ga)
```

### Comparing Cost Functions and Optimization Methods
Below we can see the accuracy of the different optimization methods (BFGS, Newton's method and Gradient Ascent) on the different cost functions (MAP or MLE).
```{r MAP and MLE results logistic regression}
# put everything together into a nice table
results_matrix <- matrix(c(acc_mle_bfgs, acc_mle_nm, acc_mle_ga,
                           acc_map_bfgs, acc_map_nm, acc_map_ga), 
                         dimnames=list(c("BFGS", "NM", "GA"), c("MLE", "MAP")), 
                         nrow=3, ncol=2)
results <- data.frame(results_matrix)
kable(results)
```

### Random-Walk Metropolis-Hastings Implementation on Reduced Data
Next, we can define a function that performs Random Walk Metropolis Hastings with a normal proposal distribution. To make it more efficient we can work with the log posterior and change the decision rule. In addition, we can pre-calculate all the normal and uniform samples and just access them later.

```{r RWMH definition}
rwmh_multivariate_log <- function(start, niter, logtarget, vcov, thinning, burnin){
    # Set current z to the initial point and calculate its log target to save computations
    z  <- start    # It's a column vector
    pz <- logtarget(start)
    # create vector deciding iterations where we record the samples
    store <- seq(from=(1+burnin), to=niter, by=thinning)
    #n_samples <- (niter - burnin) %/% thinning
    # Generate matrix containing samples. Initialize with the starting value
    samples <- matrix(0, nrow=length(store), ncol=nrow(start))
    samples[1, ] <- start
    # Generate uniform random numbers in advance, to save computation. Log them.
    log_u <- log(runif(niter))
    # Proposal is a multivariate standard normal distribution. Generate samples and
    # later on use linearity property of Gaussian distribution
    vcov <- diag(nrow(start)) %*% vcov
    normal_shift <- mvrnorm(n=niter, mu=c(0,0,0), Sigma=vcov)
    for (i in 2:niter){
        # Sample a candidate
        candidate <- z + normal_shift[i, ]
        # calculate log target of candidate and store it in case it gets accepted
        p_candidate <- logtarget(candidate)
        # use decision rule explained in blog posts
        if (log_u[i] <= p_candidate - pz){
            # Accept!
            z  <- candidate
            pz <- p_candidate
        }
        # Finally add the sample to our matrix of samples
        if (i %in% store) samples[which(store==i), ] <- z
    }
    return(samples)
}
```

Now we can add a column of $1$s to the reduced dataset created by PCA so that we can run Logistic Regression on the reduced data.
```{r reduced train logistic regression}
Xrwmh <- cbind(1, as.matrix(pca_reduced_training_data[, c(1, 2)]))
```

We run logistic regression using the BFGS optimization method on MAP. Since we need the Hessian matrix for sampling later on, we simply use the `optim` function on the unnormalized version of the posterior distribution for logistic regression.

```{r log posterior and optim logistic regression}
# up to normalizing constant
log_posterior_unnormalized <- function(beta){
    log_prior      <- -0.5*sum(beta^2)
    log_likelihood <- -sum(log(1 + exp((1 - 2*training_classes) * (Xrwmh %*% beta))))
    return(log_prior + log_likelihood)
}
# To start the algorithm more efficiently, start from MAP estimate
optim_results <- optim(c(0,0,0), function(x) - log_posterior_unnormalized(x), 
                       method="BFGS", hessian=TRUE)
start <- matrix(optim_results$par)
# Use inverse of approximate hessian matrix as vcov of normal proposal
vcov  <- solve(optim_results$hessian)
```

We're now ready to start sampling starting at the MAP estimate and using, as a proposal, the normal distribution with variance-covariance matrix given by the inverse of the approximated hessian matrix coming from the optimization routine.

```{r samples logistic regression}
samples <- rwmh_multivariate_log(start, niter=50000, log_posterior_unnormalized,
                                 vcov, thinning=1, burnin=0)
```

We can look at the trace plots and notice how the chain is mixing quickly, signalling a good exploratory behavior.

```{r, echo=FALSE}
samplesdf <- data.frame(samples) %>% mutate(iterations=row_number())
trace1 <- ggplot(data=samplesdf, aes(x=iterations, y=X1)) + geom_line()
trace2 <- ggplot(data=samplesdf, aes(x=iterations, y=X2)) + geom_line()
trace3 <- ggplot(data=samplesdf, aes(x=iterations, y=X3)) + geom_line()
grid.arrange(trace1, trace2, trace3, ncol=1)
```

Plotting the histograms of the samples for each coordinate of the parameter vector shows that RWMH is indeed sampling correctly.

```{r, echo=FALSE}
hist1 <- ggplot(data=samplesdf, aes(x=X1, stat(density))) + 
  geom_histogram(binwidth=0.05, alpha=0.5, fill="turquoise1", color="turquoise4")
hist2 <- ggplot(data=samplesdf, aes(x=X2, stat(density))) + 
  geom_histogram(binwidth=0.05, alpha=0.5, fill="turquoise1", color="turquoise4")
hist3 <- ggplot(data=samplesdf, aes(x=X3, stat(density))) + 
  geom_histogram(binwidth=0.05, alpha=0.5, fill="turquoise1", color="turquoise4")
grid.arrange(hist1, hist2, hist3, ncol=1)
```

The next thing to do is to plot the decision boundary found by BFGS together with the sampled lines. 

```{r, echo=FALSE}
samples_to_select <- 200
samples_subset <- samples[sample(1:nrow(samples), samples_to_select), ]
# Calculate slope and intercept for those samples. Then calculate x2 from x1
linecoefs <- cbind(-samples_subset[, 2]/samples_subset[, 3], - samples_subset[, 1] / samples_subset[, 3])
x2_vals <- apply(linecoefs, 1, function(row) Xrwmh[, 2]*row[1] + row[2])
# Store x2 values together with x1 values from X. Then melt to plot all lines 
dfsample_lines <- data.frame(x1=Xrwmh[, 2], x2_vals) %>% 
  gather("key", "value", -x1)
# replace diagnosis 
data_plot <- pca_reduced_training_data %>% mutate(training_classes=training_classes)
# Create dataframe containing values for the MAP line
dfmap <- data.frame(x1=Xrwmh[, 2], y=(-start[2, ]/start[3, ])*Xrwmh[, 2] + (-start[1, ]/start[3, ]))
ggplot() +
  geom_point(data=data_plot, aes(x=X1, y=X2, color=as.factor(training_classes))) +    # Dataset scatter plot
  coord_cartesian(xlim=c(-550, -300), ylim=c(-550, -300)) + 
  geom_line(data=dfsample_lines, aes(x=x1, y=value, group=key), alpha=0.1, color="grey50") +                     
  geom_line(data=dfmap, aes(x=x1, y=y), color="black") +  # MAP line
  labs(color="Class", title="Sample Decision Boundaries") + 
  theme(plot.title=element_text(hjust=0.5, size=20))
```

Finally, we can look at the performance of logistic regression on this reduced dataset. As a decision rule we just use a threshold.
```{r embedded accuracy logistic regression}
Xtest_embedded <- cbind(1, as.matrix(embedded_test_data[, c(1, 2)]))
lr_embedded_preds <- round(1.0 / (1.0 + exp(-Xtest_embedded %*% start)))
calc_accuracy(test_classes, lr_embedded_preds)
```

\newpage
# Conclusion

Merge all predictions
```{r}
id <- seq(length(id_test))
all_pred <- cbind(id,numeric_test_labels,pred_naive,pred_svm)
colnames(all_pred) <- c('id','actual','naive','svm')
all_pred[all_pred==-1] <-0
all_pred %<>% as.data.frame()
```

```{r}
confusion_plot <- function(actual,predicted){
  confusion_matrix <- as.data.frame(table(actual,predicted))
  g <-ggplot(confusion_matrix,aes(x=actual,y=predicted))+
    geom_tile(aes(fill=Freq))+
    geom_text(aes(label=sprintf("%1.0f", Freq)),color="white",fontface="bold")+
   labs(x="Actual class",y="Predicted class")+
    theme_minimal()
  return(g)
}
```

```{r}
errors <- all_pred %>% 
  mutate(naive=naive==actual,
         svm=svm==actual,
         logistic=yhat_map_bfgs==actual) %>% 
  dplyr::select(-'actual') %>% 
  melt(id='id')
```

Visualize which are the observations that the models missclassify.
```{r}
ggplot(errors,aes(x=id,y=variable,fill=value))+
  geom_raster()+
  theme_minimal()
```
