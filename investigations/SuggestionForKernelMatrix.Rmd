---
title: "Kernel Matrix"
author: "Mauro Camara Escudero"
date: "11/18/2019"
output: html_document
---
<!--- For HTML Only --->
`r if (!knitr:::is_latex_output()) '
$\\DeclareMathOperator*{\\argmin}{arg\\,min}$
$\\DeclareMathOperator*{\\argmax}{arg\\,max}$
$\\newcommand{\\var}{\\mathrm{Var}}$
'`
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{color}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{algorithm2e}
\newcommand{\def}{\overset{\text{def}}{:=}}
\newcommand{\lop}{\mathcal{L}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\norm}[1]{||#1||}
\newcommand{\vx}{\vect{x}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vz}{\vect{z}}
\newcommand{\ve}{\vect{e}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\xhat}{\widehat{x}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vr}{\vect{r}}
\newcommand{\vphi}{\vect{\phi}}
\newcommand{\vf}{\vect{f}}
\newcommand{\vY}{\vect{Y}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vw}{\vect{w}}
\newcommand{\thetahatis}{\widehat{\theta}^{(s)}_i}
\newcommand{\thetahat}[1]{\widehat{\theta}^{(#1)}_i}
\newcommand{\vm}{\vect{m}}
\newcommand{\redmath}[1]{\mathbin{\textcolor{red}{\vect{#1}}}}
\newcommand{\redtext}[1]{\textcolor{red}{\vect{#1}}}
\newcommand{\vzero}{\vect{0}}
\newcommand{\vt}{\vect{t}}
\newcommand{\linearpredictor}{\vx_i^T\vbeta}
\newcommand{\vmu}{\vect{\mu}}
\newcommand{\vnu}{\vect{\nu}}
\newcommand{\Var}{\text{Var}}
\newcommand{\veta}{\vect{\eta}}
\newcommand{\vbeta}{\vect{\beta}}
\newcommand{\vepsilon}{\vect{\epsilon}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\vdelta}{\vect{\delta}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\vxi}{\vect{\xi}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vlambda}{\vect{\lambda}}
\newcommand{\dataset}{\mathcal{D}}
\newcommand{\vS}{\vect{S}}
\newcommand{\sample}{\vz^{(l)}}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}
\newcommand{\sol}[1]{\vx^{(#1)}}
\newcommand{\qtext}[1]{\quad\quad \text{#1}}
\newcommand{\vtheta}{\vect{\theta}}
\newcommand{\bi}[1]{\textbf{\textit{#1}}}
\newcommand{\iid}{\overset{\text{i.i.d.}}{\sim}}
\newcommand{\uniform}{\mathcal{U}(0, 1)}
\newcommand{\qimplies}{\quad\Longrightarrow\quad}
\newcommand{\tp}{\tilde{p}}
\newcommand{\nul}{\Theta^{(0)}}
\newcommand{\alter}{\Theta^{(1)}}
\newcommand{\const}{\mathcal{Z}}
\newcommand{\tq}{\tilde{q}}
\newcommand{\vxhat}{\widehat{\vx}}
\newcommand{\tvx}{\widetilde{\vx}}
\newcommand{\tr}{\tilde{r}}
\newcommand{\like}{\mathcal{L}}
\newcommand{\kl}[2]{\text{KL}(#1\,\,||\,\,#2)}
\newcommand{\logit}[1]{\log\left(\frac{#1}{1-#1}\right)}
\newcommand{\elbo}[1]{\text{elbo}(#1)}
\newcommand{\eval}{\biggr\rvert}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\class}{\mathcal{C}}
\newcommand{\infor}{\mathcal{I}}
\newcommand{\variance}{\text{Var}}
\newcommand{\delement}[2]{\vx_{#1}^\top\vx_{#1} - 2\vx_{#1}^\top \vx_{#2} + \vx_{#2}^\top\vx_{#2}}
\newcommand{\vSigma}{\vect{\Sigma}}
\newcommand{\lp}[3]{
    \begin{equation*}
    \begin{alignat}{2}
    &\!\min       &\qquad& #1\\
    &\text{s.t.} &      & #2\\
    &                  &      & #3
    \end{alignat}
    \end{equation*}
}
\newcommand{\lpmax}[3]{
    \begin{equation*}
    \begin{alignat}{2}
    &\!\max       &\qquad& #1\\
    &\text{s.t.} &      & #2\\
    &                  &      & #3
    \end{alignat}
    \end{equation*}
}
\newcommand{\nrm}[1]{\parallel #1 \parallel}
\newcommand{\dot}[2]{\vx_{#1}^\top\vx_{#2}}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- ## Get data -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- library(ggplot2) -->
<!-- source("data_preparation.R") -->
<!-- df <- get_data() -->
<!-- # Restrict to only n datapoints -->
<!-- n = 1000 -->
<!-- df <- df[1:n, ] -->
<!-- ``` -->

<!-- Plot the magnitude over time -->
<!-- ```{r} -->
<!-- ggplot(data=df, aes(x=time, y=magnit)) +  -->
<!--   geom_point() -->
<!-- ``` -->


Suppose that we have a matrix $X\in\Rbb^{n\times d}$ whose rows represent one observation of $d$ features, and whose columns represent all $n$ observations of a particular feature. 
$$
\begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1d} \\
x_{21} & x_{22} & \cdots & x_{2d} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nd}
\end{pmatrix} = 
\begin{pmatrix}
\vx_1^\top \\
\vx_2^\top \\
\vdots \\
\vx_n^\top
\end{pmatrix} \qquad \text{where } \vx_i\in\Rbb^{d\times 1} \quad \forall \, i\in\{1, \ldots, n\}
$$
We want to use the kernel function
$$
k(\vx_i, \vx_j) := \exp\left(- \frac{\parallel\vx_i - \vx_j\parallel^2}{\sigma^2}\right)
$$
to create the kernel matrix $K$ whose entries are $[K]_{ij} :=k(\vx_i, \vx_j)$. To vectorize this operation, we aim to create a matrix containing all the squared norms of the difference between the vectors
$$
D = 
\begin{pmatrix}
  \nrm{\vx_1 - \vx_1}^2 & \nrm{\vx_1 - \vx_2}^2 & \cdots & \nrm{\vx_1 - \vx_n}^2 \\
  \nrm{\vx_2 - \vx_1}^2 & \nrm{\vx_2 - \vx_2}^2 & \cdots & \nrm{\vx_2 - \vx_n}^2 \\
  \vdots             & \vdots             & \ddots & \vdots \\
  \nrm{\vx_n - \vx_1}^2 & \nrm{\vx_n - \vx_2}^2 & \cdots & \nrm{\vx_n - \vx_n}^2
\end{pmatrix}
$$
so that then we can simply run this:
```{r eval=FALSE}
exp(- D / sigma_sq)
```
Notice that we can rewrite the norm of the difference between two vectors $\vx_i$ and $\vx_j$ as follows
$$
\nrm{\vx_i - \vx_j}^2 = (\vx_i - \vx_j)^\top (\vx_i - \vx_j) = \vx_i^\top\vx_i - 2\vx_i^\top \vx_j + \vx_j^\top\vx_j
$$
Broadcasting this, we can rewrite the matrix $D$ as
$$
D =
\begin{pmatrix}
  \delement{1}{1} & \quad\delement{1}{2} & \quad\cdots & \quad\delement{1}{n}\\
  \delement{2}{1} & \quad\delement{2}{2} & \quad\cdots & \quad\delement{2}{n}\\
  \vdots          & \quad\vdots          & \quad\ddots & \quad\vdots \\
  \delement{n}{1} & \quad\delement{n}{2} & \quad\cdots & \quad\delement{n}{n}
\end{pmatrix}
$$
This can now be split into three matrices, where we can notice that the third matrix is nothing but the transpose of the first.
$$
D =
\begin{pmatrix}
\vx_1^\top\vx_1 & \vx_1^\top\vx_1 & \cdots & \vx_1^\top\vx_1\\
\vx_2^\top\vx_2 & \vx_2^\top\vx_2 & \cdots & \vx_2^\top\vx_2 \\
\vdots          & \vdots          & \ddots & \vdots\\
\vx_n^\top\vx_n & \vx_n^\top\vx_n & \cdots & \vx_n^\top\vx_n
\end{pmatrix}
-2
\begin{pmatrix}
  \dot{1}{1} & \dot{1}{2} & \cdots & \dot{1}{n} \\
  \dot{2}{1} & \dot{2}{2} & \cdots & \dot{2}{n} \\
  \vdots     & \vdots     & \ddots & \vdots \\
  \dot{n}{1} & \dot{n}{2} & \cdots & \dot{n}{n}
\end{pmatrix}
+
\begin{pmatrix}
\dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n} \\
\dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n} \\
\vdots     & \vdots     & \ddots & \vdots \\
\dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n} 
\end{pmatrix}
$$
The key is not to notice that we can obtain the middle matrix from $X$ with a simple operation:
$$
XX^\top = 
\begin{pmatrix}
  \vx_1^\top \\
  \vx_2^\top \\
  \vdots \\
  \vx_n^\top
\end{pmatrix}
\begin{pmatrix}
\vx_1 & \vx_2 & \cdots & \vx_n 
\end{pmatrix}
=
\begin{pmatrix}
  \dot{1}{1} & \dot{1}{2} & \cdots & \dot{1}{n} \\
  \dot{2}{1} & \dot{2}{2} & \cdots & \dot{2}{n} \\
  \vdots     & \vdots     & \ddots & \vdots \\
  \dot{n}{1} & \dot{n}{2} & \cdots & \dot{n}{n}
\end{pmatrix}
$$
and then the first matrix can be obtained as follows
$$
\begin{pmatrix}
  1 \\
  1 \\
  \vdots \\
  1
\end{pmatrix}_{n\times 1}
\begin{pmatrix}
  \dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n}
\end{pmatrix}_{1\times n}
= 
\begin{pmatrix}
\vx_1^\top\vx_1 & \vx_1^\top\vx_1 & \cdots & \vx_1^\top\vx_1\\
\vx_2^\top\vx_2 & \vx_2^\top\vx_2 & \cdots & \vx_2^\top\vx_2 \\
\vdots          & \vdots          & \ddots & \vdots\\
\vx_n^\top\vx_n & \vx_n^\top\vx_n & \cdots & \vx_n^\top\vx_n
\end{pmatrix}
$$
where
$$
\text{diag}\left[
\begin{pmatrix}
  \dot{1}{1} & \dot{1}{2} & \cdots & \dot{1}{n} \\
  \dot{2}{1} & \dot{2}{2} & \cdots & \dot{2}{n} \\
  \vdots     & \vdots     & \ddots & \vdots \\
  \dot{n}{1} & \dot{n}{2} & \cdots & \dot{n}{n}
\end{pmatrix}
\right]
=
\begin{pmatrix}
  \dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n}
\end{pmatrix}_{1\times n}
$$
Therefore we can write a function to calculate the `rbf` kernel matrix as follows in `R`:
```{r}
kernel_matrix <- function(X, sigmasq){
  # Find second matrix
  Y <- tcrossprod(X)
  # Find first matrix (whose transpose is the second)
  Z <- matrix(1, nrow(X)) %*% diag(Y)
  return(exp(-(Z - 2*Y + t(Z)) / sigmasq))
}
```
We can try this out on a simple matrix
```{r}
X <- matrix(1:20, 4, 5)
X
```
which gives
```{r}
kernel_matrix(X, 1)
```

and it makes sense because it has `1` on the diagonals, it is symmetric and the values corresponding to the furthest away points have the smallest values.

The advantage of this technique is that we can get the median of the pairwise distances as a byproduct

```{r}
kernel_matrix <- function(X){
  # Find second matrix
  Y <- tcrossprod(X)
  # Find first matrix (whose transpose is the second)
  Z <- matrix(1, nrow(X)) %*% diag(Y)
  return(exp(-(Z - 2*Y + t(Z)) / median(Y)))
}
kernel_matrix(X)
```

















