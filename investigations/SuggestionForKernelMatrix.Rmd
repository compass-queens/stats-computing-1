---
title: "Kernel Matrix"
author: "Mauro Camara Escudero"
date: "11/18/2019"
output: html_document
---
<!--- For HTML Only --->
`r if (!knitr:::is_latex_output()) '
$\\DeclareMathOperator*{\\argmin}{arg\\,min}$
$\\DeclareMathOperator*{\\argmax}{arg\\,max}$
$\\newcommand{\\var}{\\mathrm{Var}}$
'`
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{color}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{algorithm2e}
\newcommand{\def}{\overset{\text{def}}{:=}}
\newcommand{\lop}{\mathcal{L}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\norm}[1]{||#1||}
\newcommand{\vx}{\vect{x}}
\newcommand{\vb}{\vect{b}}
\newcommand{\vy}{\vect{y}}
\newcommand{\vz}{\vect{z}}
\newcommand{\ve}{\vect{e}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\xhat}{\widehat{x}}
\newcommand{\vc}{\vect{c}}
\newcommand{\vr}{\vect{r}}
\newcommand{\vphi}{\vect{\phi}}
\newcommand{\vf}{\vect{f}}
\newcommand{\vY}{\vect{Y}}
\newcommand{\vX}{\vect{X}}
\newcommand{\vw}{\vect{w}}
\newcommand{\thetahatis}{\widehat{\theta}^{(s)}_i}
\newcommand{\thetahat}[1]{\widehat{\theta}^{(#1)}_i}
\newcommand{\vm}{\vect{m}}
\newcommand{\redmath}[1]{\mathbin{\textcolor{red}{\vect{#1}}}}
\newcommand{\redtext}[1]{\textcolor{red}{\vect{#1}}}
\newcommand{\vzero}{\vect{0}}
\newcommand{\vt}{\vect{t}}
\newcommand{\linearpredictor}{\vx_i^T\vbeta}
\newcommand{\vmu}{\vect{\mu}}
\newcommand{\vnu}{\vect{\nu}}
\newcommand{\Var}{\text{Var}}
\newcommand{\veta}{\vect{\eta}}
\newcommand{\vbeta}{\vect{\beta}}
\newcommand{\vepsilon}{\vect{\epsilon}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\vdelta}{\vect{\delta}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\vxi}{\vect{\xi}}
\newcommand{\vu}{\vect{u}}
\newcommand{\vW}{\vect{W}}
\newcommand{\vlambda}{\vect{\lambda}}
\newcommand{\dataset}{\mathcal{D}}
\newcommand{\vS}{\vect{S}}
\newcommand{\sample}{\vz^{(l)}}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}
\newcommand{\sol}[1]{\vx^{(#1)}}
\newcommand{\qtext}[1]{\quad\quad \text{#1}}
\newcommand{\vtheta}{\vect{\theta}}
\newcommand{\bi}[1]{\textbf{\textit{#1}}}
\newcommand{\iid}{\overset{\text{i.i.d.}}{\sim}}
\newcommand{\uniform}{\mathcal{U}(0, 1)}
\newcommand{\qimplies}{\quad\Longrightarrow\quad}
\newcommand{\tp}{\tilde{p}}
\newcommand{\nul}{\Theta^{(0)}}
\newcommand{\alter}{\Theta^{(1)}}
\newcommand{\const}{\mathcal{Z}}
\newcommand{\tq}{\tilde{q}}
\newcommand{\vxhat}{\widehat{\vx}}
\newcommand{\tvx}{\widetilde{\vx}}
\newcommand{\tr}{\tilde{r}}
\newcommand{\like}{\mathcal{L}}
\newcommand{\kl}[2]{\text{KL}(#1\,\,||\,\,#2)}
\newcommand{\logit}[1]{\log\left(\frac{#1}{1-#1}\right)}
\newcommand{\elbo}[1]{\text{elbo}(#1)}
\newcommand{\hX}{\widehat{X}}
\newcommand{\eval}{\biggr\rvert}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\vk}{\vect{k}}
\newcommand{\vK}{\vect{K}}
\newcommand{\class}{\mathcal{C}}
\newcommand{\infor}{\mathcal{I}}
\newcommand{\variance}{\text{Var}}
\newcommand{\delement}[2]{\vx_{#1}^\top\vx_{#1} - 2\vx_{#1}^\top \vx_{#2} + \vx_{#2}^\top\vx_{#2}}
\newcommand{\vSigma}{\vect{\Sigma}}
\newcommand{\hvx}{\widehat{\vx}}
\newcommand{\lp}[3]{
    \begin{equation*}
    \begin{alignat}{2}
    &\!\min       &\qquad& #1\\
    &\text{s.t.} &      & #2\\
    &                  &      & #3
    \end{alignat}
    \end{equation*}
}
\newcommand{\lpmax}[3]{
    \begin{equation*}
    \begin{alignat}{2}
    &\!\max       &\qquad& #1\\
    &\text{s.t.} &      & #2\\
    &                  &      & #3
    \end{alignat}
    \end{equation*}
}
\newcommand{\nrm}[1]{\parallel #1 \parallel}
\newcommand{\dot}[2]{\vx_{#1}^\top\vx_{#2}}
\newcommand{\dothat}[2]{\hvx_{#1}^\top\vx_{#2}}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Suppose that we have a matrix $X\in\Rbb^{n\times d}$ whose rows represent one observation of $d$ features, and whose columns represent all $n$ observations of a particular feature. 
$$
\begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1d} \\
x_{21} & x_{22} & \cdots & x_{2d} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nd}
\end{pmatrix} = 
\begin{pmatrix}
\vx_1^\top \\
\vx_2^\top \\
\vdots \\
\vx_n^\top
\end{pmatrix} \qquad \text{where } \vx_i\in\Rbb^{d\times 1} \quad \forall \, i\in\{1, \ldots, n\}
$$
We want to use the kernel function
$$
k(\vx_i, \vx_j) := \exp\left(- \frac{\parallel\vx_i - \vx_j\parallel^2}{\sigma^2}\right)
$$
to create the kernel matrix $K$ whose entries are $[K]_{ij} :=k(\vx_i, \vx_j)$. To vectorize this operation, we aim to create a matrix containing all the squared norms of the difference between the vectors
$$
D = 
\begin{pmatrix}
  \nrm{\vx_1 - \vx_1}^2 & \nrm{\vx_1 - \vx_2}^2 & \cdots & \nrm{\vx_1 - \vx_n}^2 \\
  \nrm{\vx_2 - \vx_1}^2 & \nrm{\vx_2 - \vx_2}^2 & \cdots & \nrm{\vx_2 - \vx_n}^2 \\
  \vdots             & \vdots             & \ddots & \vdots \\
  \nrm{\vx_n - \vx_1}^2 & \nrm{\vx_n - \vx_2}^2 & \cdots & \nrm{\vx_n - \vx_n}^2
\end{pmatrix}
$$
so that then we can simply run this:
```{r eval=FALSE}
exp(- D / sigma_sq)
```
Notice that we can rewrite the norm of the difference between two vectors $\vx_i$ and $\vx_j$ as follows
$$
\nrm{\vx_i - \vx_j}^2 = (\vx_i - \vx_j)^\top (\vx_i - \vx_j) = \vx_i^\top\vx_i - 2\vx_i^\top \vx_j + \vx_j^\top\vx_j
$$
Broadcasting this, we can rewrite the matrix $D$ as
$$
D =
\begin{pmatrix}
  \delement{1}{1} & \quad\delement{1}{2} & \quad\cdots & \quad\delement{1}{n}\\
  \delement{2}{1} & \quad\delement{2}{2} & \quad\cdots & \quad\delement{2}{n}\\
  \vdots          & \quad\vdots          & \quad\ddots & \quad\vdots \\
  \delement{n}{1} & \quad\delement{n}{2} & \quad\cdots & \quad\delement{n}{n}
\end{pmatrix}
$$
This can now be split into three matrices, where we can notice that the third matrix is nothing but the transpose of the first.
$$
D =
\begin{pmatrix}
\vx_1^\top\vx_1 & \vx_1^\top\vx_1 & \cdots & \vx_1^\top\vx_1\\
\vx_2^\top\vx_2 & \vx_2^\top\vx_2 & \cdots & \vx_2^\top\vx_2 \\
\vdots          & \vdots          & \ddots & \vdots\\
\vx_n^\top\vx_n & \vx_n^\top\vx_n & \cdots & \vx_n^\top\vx_n
\end{pmatrix}
-2
\begin{pmatrix}
  \dot{1}{1} & \dot{1}{2} & \cdots & \dot{1}{n} \\
  \dot{2}{1} & \dot{2}{2} & \cdots & \dot{2}{n} \\
  \vdots     & \vdots     & \ddots & \vdots \\
  \dot{n}{1} & \dot{n}{2} & \cdots & \dot{n}{n}
\end{pmatrix}
+
\begin{pmatrix}
\dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n} \\
\dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n} \\
\vdots     & \vdots     & \ddots & \vdots \\
\dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n} 
\end{pmatrix}
$$
The key is not to notice that we can obtain the middle matrix from $X$ with a simple operation:
$$
XX^\top = 
\begin{pmatrix}
  \vx_1^\top \\
  \vx_2^\top \\
  \vdots \\
  \vx_n^\top
\end{pmatrix}
\begin{pmatrix}
\vx_1 & \vx_2 & \cdots & \vx_n 
\end{pmatrix}
=
\begin{pmatrix}
  \dot{1}{1} & \dot{1}{2} & \cdots & \dot{1}{n} \\
  \dot{2}{1} & \dot{2}{2} & \cdots & \dot{2}{n} \\
  \vdots     & \vdots     & \ddots & \vdots \\
  \dot{n}{1} & \dot{n}{2} & \cdots & \dot{n}{n}
\end{pmatrix}
$$
and then the first matrix can be obtained as follows
$$
\begin{pmatrix}
  1 \\
  1 \\
  \vdots \\
  1
\end{pmatrix}_{n\times 1}
\begin{pmatrix}
  \dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n}
\end{pmatrix}_{1\times n}
= 
\begin{pmatrix}
\vx_1^\top\vx_1 & \vx_1^\top\vx_1 & \cdots & \vx_1^\top\vx_1\\
\vx_2^\top\vx_2 & \vx_2^\top\vx_2 & \cdots & \vx_2^\top\vx_2 \\
\vdots          & \vdots          & \ddots & \vdots\\
\vx_n^\top\vx_n & \vx_n^\top\vx_n & \cdots & \vx_n^\top\vx_n
\end{pmatrix}
$$
where
$$
\text{diag}\left[
\begin{pmatrix}
  \dot{1}{1} & \dot{1}{2} & \cdots & \dot{1}{n} \\
  \dot{2}{1} & \dot{2}{2} & \cdots & \dot{2}{n} \\
  \vdots     & \vdots     & \ddots & \vdots \\
  \dot{n}{1} & \dot{n}{2} & \cdots & \dot{n}{n}
\end{pmatrix}
\right]
=
\begin{pmatrix}
  \dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n}
\end{pmatrix}_{1\times n}
$$
Therefore we can write a function to calculate the `rbf` kernel matrix as follows in `R`:
```{r}
kernel_matrix <- function(X, sigmasq){
  # Find second matrix
  Y <- tcrossprod(X)
  # Find first matrix (whose transpose is the second)
  Z <- matrix(1, nrow(X)) %*% diag(Y)
  return(exp(-(Z - 2*Y + t(Z)) / sigmasq))
}
```
We can try this out on a simple matrix
```{r}
X <- matrix(1:20, 4, 5)
X
```
which gives
```{r}
kernel_matrix(X, 1)
```

and it makes sense because it has `1` on the diagonals, it is symmetric and the values corresponding to the furthest away points have the smallest values.

The advantage of this technique is that we can get the median of the pairwise distances as a byproduct

```{r}
kernel_matrix <- function(X){
  # Find second matrix
  Y <- tcrossprod(X)
  # Find first matrix (whose transpose is the second)
  Z <- matrix(1, nrow(X)) %*% diag(Y)
  return(exp(-(Z - 2*Y + t(Z)) / median(Y)))
}
kernel_matrix(X)
```

## Making predictions
Recall that our formula for predicting the output of a new input $\hvx$ is
$$
f(\hvx) = \vk(\vK + \lambda I)^{-1}\vy
$$
where 
$$
\vk:=\left(k(\hvx, \vx_1), \ldots, k(\hvx, \vx_n)\right)\in\Rbb^{1\times n}
$$
Now suppose that instead we get a whole bunch of testing data points
$$
\hX:=
\begin{pmatrix}
\hvx_1^\top \\
\hvx_2^\top \\
\vdots \\
\hvx_m^\top
\end{pmatrix}\in\Rbb^{m\times d}
$$
Then we can actually construct a vector of predictions as follows
$$
\begin{pmatrix}
f(\hvx_1) \\
f(\hvx_2) \\
\vdots \\
f(\hvx_m)
\end{pmatrix}
= 
\begin{pmatrix}
  \vk_1(\vK + \lambda I)^{-1}\vy \\
  \vk_2(\vK + \lambda I)^{-1}\vy \\
  \vdots \\
  \vk_m(\vK + \lambda I)^{-1}\vy
\end{pmatrix}
=
\begin{pmatrix}
  \vk_1 \\
  \vk_2 \\
  \vdots \\
  \vk_m
\end{pmatrix}
(\vK + \lambda I)^{-1}\vy=: \widetilde{\vK}(\vK + \lambda I)^{-1}\vy
$$
where 
$$
\vk_i := \left(k(\hvx_i, \vx_1), \ldots, k(\hvx_i, \vx_n)\right) \qquad \text{for  } i\in\{1, \ldots, m\}
$$
and
$$
\widetilde{\vK} := 
\begin{pmatrix}
  \vk_1 \\
  \vk_2 \\
  \vdots \\
  \vk_m
\end{pmatrix} \in\Rbb^{m\times n}
$$
with entries
$$
[\widetilde{\vK}]_{ij} = k(\hvx_i, \vx_j)
$$
In other words, we need to be able to modify our algorithm to give us the following matrix
$$
\widetilde{D} = 
\begin{pmatrix}
  \nrm{\hvx_1 - \vx_1}^2 & \nrm{\hvx_1 - \vx_2}^2 & \cdots & \nrm{\hvx_1 - \vx_n}^2\\
  \nrm{\hvx_2 - \vx_1}^2 & \nrm{\hvx_2 - \vx_2}^2 & \cdots & \nrm{\hvx_2 - \vx_n}^2\\
  \vdots          & \vdots                  & \ddots & \vdots \\
  \nrm{\hvx_m - \vx_1}^2 & \nrm{\hvx_m - \vx_2}^2 & \cdots & \nrm{\hvx_m - \vx_n}^2
\end{pmatrix}
$$
which leads to 
$$
\widetilde{D} =
\begin{pmatrix}
\hvx_1^\top\hvx_1 & \hvx_1^\top\hvx_1 & \cdots & \hvx_1^\top\hvx_1\\
\hvx_2^\top\hvx_2 & \hvx_2^\top\hvx_2 & \cdots & \hvx_2^\top\hvx_2 \\
\vdots          & \vdots          & \ddots & \vdots\\
\hvx_n^\top\hvx_n & \hvx_n^\top\hvx_n & \cdots & \hvx_n^\top\hvx_n
\end{pmatrix}
-2
\begin{pmatrix}
  \dothat{1}{1} & \dothat{1}{2} & \cdots & \dothat{1}{n} \\
  \dothat{2}{1} & \dothat{2}{2} & \cdots & \dothat{2}{n} \\
  \vdots        & \vdots        & \ddots & \vdots \\
  \dothat{n}{1} & \dothat{n}{2} & \cdots & \dothat{n}{n}
\end{pmatrix}
+
\begin{pmatrix}
\dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n} \\
\dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n} \\
\vdots     & \vdots     & \ddots & \vdots \\
\dot{1}{1} & \dot{2}{2} & \cdots & \dot{n}{n} 
\end{pmatrix}
$$
Luckily, we can do this by adjusting our original `kernel_matrix` function
```{r}
kernel_matrix <- function(X, Y=NULL, sigmasq=NULL){
  if (is.null(Y)){
    Y <- X  # TODO: This might be risky for big matrices. How does R copy work? Maybe split the code into two?
  }
  n <- nrow(X)
  m <- nrow(Y)
  # Find three matrices above
  Xnorm <- matrix(apply(X^2, 1, sum), n, m)
  Ynorm <- matrix(apply(Y^2, 1, sum), n, m, byrow=TRUE)
  XY <- tcrossprod(X, Y)
  # TODO: In this case do we use median of XY? If we are using it for prediction
  # I think sigmasq should be found only from training set, not from testing set.
  # Therefore we should pass sigmasq as a parameter in this function, and should be calculated 
  # beforehand.
  if (is.null(sigmasq)) {
    # We calculate sigmasq from training data and return it to be used for prediction
    sigmasq <- median(XY)
    return(list(
        exp(-(Xnorm - 2*XY + Ynorm) / sigmasq),
        sigmasq
        )
      )
  } else {
    return(exp(-(Xnorm - 2*XY + Ynorm) / sigmasq))
  }
}
```

At this point we're ready to construct a prediction function.
```{r}
predict <- function(Xhat, X, y, reg_coef=0){
  # find K tilde and K
  Kstuff <- kernel_matrix(X)
  K <- Kstuff[[1]]
  sigmasq <- Kstuff[[2]]
  Ktilde <- kernel_matrix(Xhat, X, sigmasq = sigmasq)
  # Invert K + lambda*I where lambda=reg_coef and use Ktilde*(K+lambdaI)^{-1}y
  return(Ktilde %*% solve(K + reg_coef*diag(nrow(X))) %*% y)
}
```


## Get real data and perform regression
For now we only use depth as a feature. Also, we need one column of ones.

```{r message=FALSE, warning=FALSE}
library(ggplot2)
source("data_preparation.R")
# Restrict to only n datapoints, use only depth
n = 1000
m = 200
df <- get_data()[1:n, c("depth", "magnit")]
X <- as.matrix(df$depth)
y <- as.matrix(df$magnit)
# Get predictions
Xhat <- as.matrix(seq(0, 1, length.out=m)) #X
yhat <- predict(Xhat, X, y, reg_coef=10)
df_pred <- data.frame(depth=Xhat, pred=yhat)
# Plot stuff
ggplot(data=df, aes(x=depth, y=magnit)) +
  geom_point(alpha=0.1) + 
  geom_line(data=df_pred, aes(x=depth, y=pred), color='red')
```

We can see how it performs on the whole dataset

```{r message=FALSE, warning=FALSE}
all_data <- get_data()[, c("depth", "magnit")]
ggplot(data=all_data, aes(x=depth, y=magnit)) +
  geom_point(alpha=0.1) + 
  geom_line(data=df_pred, aes(x=depth, y=pred), color='red')
```












