---
title: "Breast Cancer Prediction"
author: "Andrea Becsek"
date: "2 January 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggplot2)
library(magrittr)
library(dplyr)
library(ggcorrplot)
library(devtools)
library(glmnet)
```

Import data
```{r}
data <- read.csv("../data/data.csv")
glimpse(data)
```
Check for NAs
```{r}

```
Remove *id* and *X*
```{r}
data %<>%  
  dplyr::select(-c(id, X))
```

Correlation plot
```{r}
corr <- data[,-1] %>% 
          cor() %>% 
           round(1)
ggcorrplot(corr, hc.order = TRUE,colors = c("#6D9EC1", "white", "#E46726"), ggtheme = ggplot2::theme_minimal)
```

Plot
```{r fig.height=40, fig.width=30}
data %>% 
  group_by(diagnosis) %>% 
  gather(key="var",value="value",-diagnosis) %>% 
  ggplot(aes(x=value))+
  facet_wrap(~var,scales='free',nrow=6)+
  geom_density(aes(fill=diagnosis),alpha=0.4)+
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position="bottom")
```


## Split data into training and test set
```{r}
# use 70% of the data for training and 30% for testing
n <- nrow(data)
train_size <- floor(0.8*n)

# sample indeces for split
train_ind <- sample(seq(n),size = train_size, replace = FALSE)
train <- data[train_ind,]
test <- data[-train_ind,]
```

## Fit lasso
```{r}
# fit lasso
y <- train$diagnosis
X <- train %>% select(-diagnosis) %>% as.matrix()
model_lasso <- glmnet(x=X,y=y, family = "binomial")

# plot of the coefficient paths, excluding the intercept
coefs <- coef(model_lasso)
plot(model_lasso,xvar = "lambda",label=TRUE)
```

Choosing a model using cross-validation
```{r}
cv_lasso <- cv.glmnet(X,y, nfolds = 10, family="binomial",type.measure = "class")

plot(cv_lasso)
```

The selected lambdas are
```{r}
# lambda that gives the minimum cross-validation error
cv_lasso$lambda.min
# lambda that gives the most regularized model with a cross-validation error within one standard error of the minimum.
cv_lasso$lambda.1se
```

We are going to use *lambda.min* to fit the model.
```{r}
# coefficients
coefs <- coef(cv_lasso,s="lambda.min")
# number of non-zero coefficients
sum(coefs!=0)
```

Make predictions
```{r}
y_test <- test$diagnosis
X_test <- test %>% select(-diagnosis) %>% as.matrix()
predictions <- predict(cv_lasso,newx=X_test,s="lambda.min",type="class")
```

```{r}
confusion_plot <- function(actual,predicted){
  confusion_matrix <- as.data.frame(table(actual,predicted))
  g <-ggplot(confusion_matrix,aes(x=actual,y=predicted))+
    geom_tile(aes(fill=Freq))+
    geom_text(aes(label=sprintf("%1.0f", Freq)),color="white",fontface="bold")+
   labs(x="Actual class",y="Predicted class")+
    theme_minimal()
  return(g)
}
```

Confusion matrix
```{r}
confusion_plot(y_test,predictions)
```

Naive Bayes
```{r}
install_github("andreabecsek/NaiveBayes")
library(NaiveBayes)
X <- unname(X)
X_test <- unname(X_test)

levels(y) <- c(0,1)
y %<>% matrix
mode(y) <- 'numeric'

levels(y_test) <-c(0,1)
y_test %<>% matrix
mode(y_test) <- 'numeric'

model_naive <- naive_bayes(X,y)
predictions_naive <- predict(model_naive,X_test)

confusion_plot(y_test,predictions_naive)
```


Logistic regression
```{r}
install_github("MauroCE/LogisticRegression",force=TRUE)
# add column of 1s
X <- unname(cbind(1,X))
X_test <- unname(cbind(1,X_test))
# change factor levels to B=0 and M=1

model_logistic <- LogisticRegression::logistic_regression(X,y,cost="MAP",method = "NEWTON")

predictions_logistic <- predict(model_logistic,xtest = X_test)

confusion_plot(y_test,predictions_logistic)
```



